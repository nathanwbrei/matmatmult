void gemm_8x12x8_libxsmm_sse(const double* A, const double* B, double* C) {
#ifdef __SSE3__
#ifdef __AVX__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling SSE3 code on AVX or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "0:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "1:\n\t"
                       "addq $6, %%r12\n\t"
                       "movapd 0(%%rdx), %%xmm7\n\t"
                       "movapd 16(%%rdx), %%xmm8\n\t"
                       "movapd 32(%%rdx), %%xmm9\n\t"
                       "movapd 64(%%rdx), %%xmm10\n\t"
                       "movapd 80(%%rdx), %%xmm11\n\t"
                       "movapd 96(%%rdx), %%xmm12\n\t"
                       "movapd 128(%%rdx), %%xmm13\n\t"
                       "movapd 144(%%rdx), %%xmm14\n\t"
                       "movapd 160(%%rdx), %%xmm15\n\t"
                       "movddup 0(%%rsi), %%xmm0\n\t"
                       "movddup 96(%%rsi), %%xmm1\n\t"
                       "movddup 192(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 8(%%rsi), %%xmm0\n\t"
                       "movddup 104(%%rsi), %%xmm1\n\t"
                       "movddup 200(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 16(%%rsi), %%xmm0\n\t"
                       "movddup 112(%%rsi), %%xmm1\n\t"
                       "movddup 208(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 24(%%rsi), %%xmm0\n\t"
                       "movddup 120(%%rsi), %%xmm1\n\t"
                       "movddup 216(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 32(%%rsi), %%xmm0\n\t"
                       "movddup 128(%%rsi), %%xmm1\n\t"
                       "movddup 224(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 40(%%rsi), %%xmm0\n\t"
                       "movddup 136(%%rsi), %%xmm1\n\t"
                       "movddup 232(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 48(%%rsi), %%xmm0\n\t"
                       "movddup 144(%%rsi), %%xmm1\n\t"
                       "movddup 240(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movddup 56(%%rsi), %%xmm0\n\t"
                       "movddup 152(%%rsi), %%xmm1\n\t"
                       "movddup 248(%%rsi), %%xmm2\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm7\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm10\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm13\n\t"
                       "movapd 16(%%rdi), %%xmm3\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm8\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm11\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm14\n\t"
                       "movapd 32(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movapd %%xmm3, %%xmm4\n\t"
                       "mulpd %%xmm0, %%xmm3\n\t"
                       "addpd %%xmm3, %%xmm9\n\t"
                       "movapd %%xmm4, %%xmm5\n\t"
                       "mulpd %%xmm1, %%xmm4\n\t"
                       "addpd %%xmm4, %%xmm12\n\t"
                       "mulpd %%xmm2, %%xmm5\n\t"
                       "addpd %%xmm5, %%xmm15\n\t"
                       "movapd %%xmm7, 0(%%rdx)\n\t"
                       "movapd %%xmm8, 16(%%rdx)\n\t"
                       "movapd %%xmm9, 32(%%rdx)\n\t"
                       "movapd %%xmm10, 64(%%rdx)\n\t"
                       "movapd %%xmm11, 80(%%rdx)\n\t"
                       "movapd %%xmm12, 96(%%rdx)\n\t"
                       "movapd %%xmm13, 128(%%rdx)\n\t"
                       "movapd %%xmm14, 144(%%rdx)\n\t"
                       "movapd %%xmm15, 160(%%rdx)\n\t"
                       "addq $48, %%rdx\n\t"
                       "subq $464, %%rdi\n\t"
                       "cmpq $6, %%r12\n\t"
                       "jl 1b\n\t"
                       "1:\n\t"
                       "addq $2, %%r12\n\t"
                       "movapd 0(%%rdx), %%xmm13\n\t"
                       "movapd 64(%%rdx), %%xmm14\n\t"
                       "movapd 128(%%rdx), %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 0(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 96(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 192(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 8(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 104(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 200(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 16(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 112(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 208(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 24(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 120(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 216(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 32(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 128(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 224(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 40(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 136(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 232(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 48(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 144(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 240(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd 0(%%rdi), %%xmm3\n\t"
                       "addq $64, %%rdi\n\t"
                       "movddup 56(%%rsi), %%xmm0\n\t"
                       "mulpd %%xmm3, %%xmm0\n\t"
                       "addpd %%xmm0, %%xmm13\n\t"
                       "movddup 152(%%rsi), %%xmm1\n\t"
                       "mulpd %%xmm3, %%xmm1\n\t"
                       "addpd %%xmm1, %%xmm14\n\t"
                       "movddup 248(%%rsi), %%xmm2\n\t"
                       "mulpd %%xmm3, %%xmm2\n\t"
                       "addpd %%xmm2, %%xmm15\n\t"
                       "movapd %%xmm13, 0(%%rdx)\n\t"
                       "movapd %%xmm14, 64(%%rdx)\n\t"
                       "movapd %%xmm15, 128(%%rdx)\n\t"
                       "addq $16, %%rdx\n\t"
                       "subq $496, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 1b\n\t"
                       "addq $128, %%rdx\n\t"
                       "addq $288, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $12, %%r13\n\t"
                       "jl 0b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("Unable to compile for SSE3: " __FILE__)
#endif
}

void gemm_8x12x8_libxsmm_avx(const double* A, const double* B, double* C) {
#ifdef __AVX__
#ifdef __AVX2__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX code on AVX2 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "0:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "1:\n\t"
                       "addq $8, %%r12\n\t"
                       "vmovapd 0(%%rdx), %%ymm10\n\t"
                       "vmovapd 32(%%rdx), %%ymm11\n\t"
                       "vmovapd 64(%%rdx), %%ymm12\n\t"
                       "vmovapd 96(%%rdx), %%ymm13\n\t"
                       "vmovapd 128(%%rdx), %%ymm14\n\t"
                       "vmovapd 160(%%rdx), %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 96(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 192(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 104(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 200(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 112(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 208(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 120(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 216(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 128(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 224(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 136(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 232(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 144(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 240(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 152(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 248(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vmulpd %%ymm3, %%ymm0, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm10, %%ymm10\n\t"
                       "vmulpd %%ymm3, %%ymm1, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm12, %%ymm12\n\t"
                       "vmulpd %%ymm3, %%ymm2, %%ymm5\n\t"
                       "vaddpd %%ymm5, %%ymm14, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vmulpd %%ymm4, %%ymm0, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm11, %%ymm11\n\t"
                       "vmulpd %%ymm4, %%ymm1, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm13, %%ymm13\n\t"
                       "vmulpd %%ymm4, %%ymm2, %%ymm6\n\t"
                       "vaddpd %%ymm6, %%ymm15, %%ymm15\n\t"
                       "vmovapd %%ymm10, 0(%%rdx)\n\t"
                       "vmovapd %%ymm11, 32(%%rdx)\n\t"
                       "vmovapd %%ymm12, 64(%%rdx)\n\t"
                       "vmovapd %%ymm13, 96(%%rdx)\n\t"
                       "vmovapd %%ymm14, 128(%%rdx)\n\t"
                       "vmovapd %%ymm15, 160(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $448, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 1b\n\t"
                       "addq $128, %%rdx\n\t"
                       "addq $288, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $12, %%r13\n\t"
                       "jl 0b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("Unable to compile for AVX: " __FILE__)
#endif
}

void gemm_8x12x8_libxsmm_avx2(const double* A, const double* B, double* C) {
#ifdef __AVX2__
#ifdef __AVX512F__
#pragma message ("LIBXSMM KERNEL COMPILATION WARNING: compiling AVX2 code on AVX512 or newer architecture: " __FILE__)
#endif
  __asm__ __volatile__("movq %0, %%rdi\n\t"
                       "movq %1, %%rsi\n\t"
                       "movq %2, %%rdx\n\t"
                       "movq $0, %%r12\n\t"
                       "movq $0, %%r13\n\t"
                       "movq $0, %%r14\n\t"
                       "0:\n\t"
                       "addq $3, %%r13\n\t"
                       "movq $0, %%r12\n\t"
                       "1:\n\t"
                       "addq $8, %%r12\n\t"
                       "vmovapd 0(%%rdx), %%ymm10\n\t"
                       "vmovapd 32(%%rdx), %%ymm11\n\t"
                       "vmovapd 64(%%rdx), %%ymm12\n\t"
                       "vmovapd 96(%%rdx), %%ymm13\n\t"
                       "vmovapd 128(%%rdx), %%ymm14\n\t"
                       "vmovapd 160(%%rdx), %%ymm15\n\t"
                       "vbroadcastsd 0(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 96(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 192(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 8(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 104(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 200(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 16(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 112(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 208(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 24(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 120(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 216(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 32(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 128(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 224(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 40(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 136(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 232(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 48(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 144(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 240(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vbroadcastsd 56(%%rsi), %%ymm0\n\t"
                       "vbroadcastsd 152(%%rsi), %%ymm1\n\t"
                       "vbroadcastsd 248(%%rsi), %%ymm2\n\t"
                       "vmovapd 0(%%rdi), %%ymm3\n\t"
                       "vmovapd 32(%%rdi), %%ymm4\n\t"
                       "vfmadd231pd %%ymm3, %%ymm0, %%ymm10\n\t"
                       "vfmadd231pd %%ymm3, %%ymm1, %%ymm12\n\t"
                       "vfmadd231pd %%ymm3, %%ymm2, %%ymm14\n\t"
                       "addq $64, %%rdi\n\t"
                       "vfmadd231pd %%ymm4, %%ymm0, %%ymm11\n\t"
                       "vfmadd231pd %%ymm4, %%ymm1, %%ymm13\n\t"
                       "vfmadd231pd %%ymm4, %%ymm2, %%ymm15\n\t"
                       "vmovapd %%ymm10, 0(%%rdx)\n\t"
                       "vmovapd %%ymm11, 32(%%rdx)\n\t"
                       "vmovapd %%ymm12, 64(%%rdx)\n\t"
                       "vmovapd %%ymm13, 96(%%rdx)\n\t"
                       "vmovapd %%ymm14, 128(%%rdx)\n\t"
                       "vmovapd %%ymm15, 160(%%rdx)\n\t"
                       "addq $64, %%rdx\n\t"
                       "subq $448, %%rdi\n\t"
                       "cmpq $8, %%r12\n\t"
                       "jl 1b\n\t"
                       "addq $128, %%rdx\n\t"
                       "addq $288, %%rsi\n\t"
                       "subq $64, %%rdi\n\t"
                       "cmpq $12, %%r13\n\t"
                       "jl 0b\n\t"
                       : : "m"(A), "m"(B), "m"(C) : "rdi","rsi","rdx","r12","r13","r14","xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7","xmm8","xmm9","xmm10","xmm11","xmm12","xmm13","xmm14","xmm15");
#else
#pragma message ("Unable to compile for AVX2: " __FILE__)
#endif
}


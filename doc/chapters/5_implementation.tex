
\chapter{Implementation}
\label{chapter:implementation}

This chapter covers the structure of the algorithm generators described previously and the resulting layout of the codebase. The first section discusses some key design decisions and the constraints driving them. The subsequent sections build up the machinery used for algorithm generation, moving from low-level to high-level. The foundation is a Python abstract syntax tree for assembly programs. These basic building blocks are aggregated and parameterized to create reusable components for subtasks such as manipulating register blocks. In order to manage the complexity of generating a traversal of a matrix with both a nontrivial sparsity pattern and a nontrivial memory layout, a \emph{matrix cursor} abstraction is introduced. This raises the possibility of a more general approach to moving information from runtime to compile time. The chapter ends with a description of the various entry points into the codebase and a brief usage guide.

\section{Design Decisions}

The first major decision was the choice of output language. There were three options available. The first option was to emit plain C/C++ code annotated with pragmas, as Breuer did. The second option was to emit C/C++ code using intrinsics. The final option was to emit plain assembly. Plain assembly was chosen for the reasons discussed below:

\begin{enumerate}

\item The primary consideration was reducing the layers of indirection. Many of the necessary design decisions are not directly expressible using C semantics. While there exist pragmas to control optimizations such as loop unrolling and vectorization, they do not always combine in predictable ways. One is forced to simultaneously reason at multiple abstraction levels: the C semantics, in order to obtain a correct program; the effect of different pragmas on code generation, in order to obtain the desired assembly; and finally the performance of the assembly on the actual hardware. Since the goal of this work is to explore how low-level algorithm changes affect performance on a specific architecture, the higher-level tools -- particularly, C with pragmas -- introduce considerable noise without introducing expressiveness to compensate.

\item The second consideration was the ease of gaining knowledge through experimentation. Given a C program which heavily uses intrinsics, an optimizing compiler will most likely produce a much more performant program compared to handwritten assembly. However, the extra optimization steps can obfuscate our experiment results, weakening our understanding of how the hardware reacts to small, precise changes. This understanding directly drives our algorithm design, which has a far greater potential upside than automatic optimization alone.

\item Another consideration was the principle of least power, as articulated by Tim Berners-Lee in~\cite{PrincipleOfLeastPower}. Generating a proper C abstract syntax tree would require considerable effort, and generating templated strings would be very fragile. Neither representation could be conveniently used by tools other than the compiler. On the other hand, an assembly AST would be easy to implement, and it could be gradually extended to support higher-level semantics. Small tools may then independently interact with this AST. Such tools may range from prettyprinters, to reordering transformations, to full interpreters. Separating these concerns leads to a cleaner and more reliable architecture.

\item The final consideration was continuity with existing codebases. The \py{libxsmm} library already emits plain assembly, and it would be straightforward to reimplement its microkernel using an assembly AST. Each successive investigation would build upon this basic codebase, leading to a lower-risk software engineering process.

\end{enumerate}

The second major design decision was the choice of language for the code generator itself. The two main requirements were that the language be amenable to rapid prototyping, be well-suited as a base for lightweight \glspl{DSL}, and have convenient access to numerical libraries. Two languages closely fit this niche: Python and Julia. Of the two, Python won due to its practicality -- the current SeisSol code generation framework is already written in Python, the libraries are mature, and the code can be run directly on the cluster. 

Julia, in comparison, has several notable advantages. Firstly, it has a much stronger type system, leading to both better performance and better correctness checking. Secondly, it is homoiconic, making it very amenable to metaprogramming. Thirdly, it is JIT-compiled on top of LLVM, which allows for features such as generating and displaying assembly code straight from the REPL. 

Ultimately Julia was considered too risky because the language and libraries are changing quickly. Furthermore, the benefit of being able to mix high-level and low-level programming paradigms was cancelled out by the lack of support on the cluster: when run locally, the Julia interpreter must not emit AVX-512 instructions. The prospect of implementing the ideas explored in this work as Julia macros remains intriguing, but also well out of scope.

Two downsides to using Python emerged over the course of the work. Firstly, refactoring proved to be surprisingly time-consuming, given the lack of type safety. This was partially mitigated by leveraging the new type annotation syntax along with the mypy type checker. Use of another new but irresistible syntax, `f'-string interpolation, constrained the code's compatibility to Python 3.6, which is not installed on the cluster either.

These considerations lead to a strategic decision to keep a clear distinction between code generation and runtime, thus avoiding real metaprogramming. The code generation framework is architecture independent, written in pure Python, and emitting output in plain text. This plain text file happens to be a C++ program containing inlined AVX-512 instructions, which must be uploaded to the cluster, compiled, and run there. The upside of this approach is that it is simple and it allows the user to manually tinker with the generated C++/assembly. The downside is that it substantially increases the time and effort it takes to obtain feedback after making changes. 


\section{Code organization}




\subsection{sparsemmgen}

\texttt{sparsemmgen} is the primary entry point for the program. It requires a minimal set of parameters: the choice of algorithm generator, values of $m, n, k, lda, ldb, ldc,$ and the path to an MTX file containing the sparsity pattern. It emits a C function containing inline assembly straight to standard out. For closer control over the generators it accepts optional parameters: values of $bm, bn, bk$, the output filename, the output function name, and output format. The latter allows the user to choose between GCC, GAS, or pseudocode. 



\subsection{libxsmmproxy}

While \py{sparsemmgen} is designed to allow the user to control the algorithm generator precisely, it suffers from two drawbacks. Firstly, the burden of choosing the best algorithm and tuning it correctly is put entirely on the user. Secondly, if an existing codebase has been sufficiently optimized such that its developers would consider using the tools developed here, the integration would likely be very difficult, since the information \py{sparsemmgen} needs is unlikely to be present. Reducing the barrier to integration increases the likelihood of this tool being used.

\py{libxsmmproxy} addresses both of these points. It is a command-line program which matches the interface of \py{libxsmm_gemm_generator} exactly and can be integrated into an existing codebase by changing a path or adding a symbolic link. (No plans for creating a JIT version exist at this time.) The program analyzes its arguments and decides whether to use the \py{sparsemmgen} implementation or to simply delegate to the original \py{libxsmm_gemm_generator}.

Unfortunately, the information which the \py{libxsmm_gemm_generator} interface provides only partially overlaps with the information which \py{sparsemmgen} needs. On one hand, \py{libxsmm_gemm_generator} does not know about our generators or their parameters such as block sizes. On the other, it supports different microarchitectures, single precision, and different prefetching strategies, which this work currently does not. Thus the proxy must pattern-match for the KNL architecture, double precision, and no prefetching strategy, and delegate all other requests. Then it must conjure the generator choice and its parameters.

There are two approaches to supplying the missing information, one manual and one automatic. The manual approach is a whitelist, in pure Python, which maps specific MTX files to \py{Parameters} objects. It is advantageous insofar as it gives the user precise control, and disadvantageous insofar as it is repetitive and requires knowledge of the system. It is best combined with the automatic system described next.

In theory, automatically choosing the best generator involves determining, for each generator:
\begin{itemize}
  \item{Can the generator handle the posed problem at all?}
  \item{What is the best choice of parameters for this (problem, generator)?}
  \item{What is the expected performance of this choice?}
\end{itemize}
and then choosing the (generator, parameters) which offer the best performance. In the general case this is a combinatorial optimization problem, but in practice the matrix sizes are very small and the parameter space is very constrained, particularly when only allowing perfect tilings. Thus a brute-force approach is satisfactory for the time being. 

In the case of dense-by-sparse multiplication, a brute-force approach is not even necessary most of the time. If the number of nonzeros in the B matrix is below the limit imposed by the instruction cache, there is no reason to use any generator besides UnrolledSparse. If it is greater, UnrolledSparse is out of the question, and the controller need only choose between TiledSparse, BlockedSparse, GeneralSparse, and dense. TiledSparse and BlockedSparse are likely to perform very well on certain sparsity patterns but not in the general case, so the automatic system can default to using GeneralSparse. There will be a point at which the penalty of performing indirect jumps outweighs the saved FLOPs, in which point the system should simply use the dense algorithm. This we can determine experimentally.


\subsection{runexperiment}
The third main entry point is \texttt{runexperiment}, which generates a 

\subsection{Generators}

\subsection{Experiments}




\section{Code generation}

When designing the AST, there is an inherent conflict between choosing data representations which are optimal for human understanding, optimal for custom tools to modify the AST, or optimal for generating the assembly code. This is acerbated by Python's lack of constructor polymorphism and encapsulation. The human benefits from readability and a tower of abstractions which successively add specificity such as register length, precision, alignment, label names, and even register names. The automated tools benefit from having a guarantee than any AST node is fully articulated, while still allowing select pieces to be abstract. Finally, the assembler needs every piece of information, repeated for every instruction, in a somewhat idiosyncratic format.

The solution to this conundrum is to decouple these representations. The AST is stripped down to be nothing more than a data container with a constructor that enforces some basic completeness invariants. The fields are chosen to be as \emph{complete} and \emph{orthogonal} as possible. The goals of being simple to work with, and of having a close mapping to the resulting assembly, are both disregarded with prejudice. Responsibility for the former is moved to the DSL, and responsibility for the latter is moved to the prettyprinter. 

\subsection{Low-level abstract syntax tree}

\begin{itemize}
  \item Operands: Label interning, Registers and register arrays, MemoryAddress, Immediates. Type information: Currently attached to registers, not really used

  \item AsmStmt base class. 
  \item Data, Jump, Label, Cmp, Add
  \item Mov, Fma

\end{itemize}

\subsection{High-level abstract syntax tree}
\begin{description}
  \item Blocks. ==> enables reusable components, comments, stack traces, etc. Currently utterly lacking in type safety. Slightly better than a preprocessor, functions which make a component generally contain a bunch of asserts beforehand. Many components are simply a function which returns a block. This is how unrolling is currently implemented in the generators.

  \item Forms ==> Sort of an unhygenic macro with arguments. Somehow rearranges or transforms its sub-blocks while otherwise behaving like a block. Completely independent of the program execution state. It is possible to make this hygenic by using abstract registers and adding a register assignment step. Currently used for loops and jump tables, which shall be discussed later. Extends Block, overrides contents(). Stateless.

  \item Commands ==> not really used at the present. The goal is to express higher-level ideas which require knowledge of the program's state at runtime. This allows information from runtime to be moved to compile time, For instance, matrixcursor movement commands, loop unroll commands. Might also allow . Will be discussed later. Stateful.
\end{description}

\subsection{A domain-specific language}
  

Operands: 
Instructions: functions
Blocks: S-exprs, builders


  Sugar to decouple data models from convenient Python syntax,
    \py{vmov(zmm0, rax+22) ==> vmovapd()}. Potential to determine from parameters instead of from dsl. 


\subsection{Visitors}
Quick overview of visitor pattern. Decoupling of operations over syntax tree from tree itself. Tree only contains data, each 'verb' receives its own Python module.

\begin{itemize}
  \item Generating GAS, pretty, C++. Comments. Potential to generate C++ intrinsics.
  \item Detecting registers which have been modified
  \item Type safety, register allocation
  \item Simulation
\end{itemize}



\section{Components}

\subsection{Register blocks}
\begin{itemize}
  \item Load, store, mask
  \item Mask computation
  \item Sparse microkernel
\end{itemize}

\subsection{Microkernels}

\subsection{Cursors}

    The \emph{matrix cursor} is an abstraction that allows the code generators to cleanly traverse a sparse matrix and access its contents while only using logical coordinates. The name was chosen because of its similarities to the cursor in a text editor, which must translate movement and insert commands from 2D screen coordinates into a 1D memory location which depends on the lengths on the neighboring lines. The term 'cursor' has already been overloaded to describe a kind of iterator into a database; this is also a kind of iterator. MatrixCursor performs the following tasks:

    \begin{description}

    \item[Traversing a matrix by blocks.] MatrixCursor translates logical block coordinates \texttt{(Bki, Bni)} into a movement instruction. This instruction updates a register pointing to the start of the `active' block. The logical block coordinates may be either absolute or relative to the active block.

    \item[Optionally tracking which block is active.] Generating a GEMM is much simpler if the generator can model the state of the machine while deciding which code to emit in order to modify said state. Not all generators implemented currently do this; some emit loops which do not get simulated, and then assume that the loops maintain certain invariants. This is the start of a more theoretical discussion in Section X.XX. 

    \item[Accessing cells within a block.] MatrixCursor translates logical cell coordinates \texttt{(bki, bni)} into a \texttt{MemoryAddress} operand. Cell coordinates are relative to the top-left-corner of the active block. It also generates a comment string (which can be rendered alongside the assembly) indicating which logical coordinates lie behind the otherwise inscrutable memory address.

    \item[Handling the case of empty blocks and cells.] The generators may directly query whether a given block or cell exists. If the generators attempt to move to an empty block or access an empty cell, the MatrixCursor will throw an exception because there is no meaningful memory address which can be associated with that logical location. Pre-emptively checking for these cases makes the generation code much cleaner.

    \item[Retrieving information about the active block.] This information includes the block shape \texttt{bk, bn}, sparsity pattern, and pattern index. This is inessential, but has proven convenient for implementing nested block decompositions.

    \end{description}

    This is achieved through the following interface:
    \begin{listing}
        \begin{minted}{python}

            class Cursor:
                def has_nonzero_cell(self, 
                                     current_block: CursorLocation, 
                                     target_block: Coords, 
                                     target_cell: Coords) -> bool:
                    pass

                def has_nonzero_block(self, 
                                      current_block: CursorLocation, 
                                      dest_block: Coords) -> bool:
                    pass

                def move(self, 
                         current_block: CursorLocation, 
                         target_block: Coords) -> Tuple[AsmStmt, CursorLocation]:
                    pass

                def look(self, 
                         current_block: CursorLocation, 
                         target_block: Coords, 
                         target_cell: Coords) -> Tuple[MemoryAddress, str]:
                    pass

                def start_location(self, target_block: Coords) -> CursorLocation:
                    pass

                def get_block(self, current_block: CursorLocation, target_block: Coords) -> BlockInfo
                    pass
        \end{minted}
        \caption{MatrixCursor interface}
        \label{lst:matrixcursor_interface}
    \end{listing}

    The purpose of a matrix cursor is to separate concerns in order to keep the algorithm generation logic as clean and simple as possible. Otherwise there would be a proliferation of minor variations of the same few algorithms. The complexity which is encapsulated includes the following considerations:

    \begin{description}
        \item[Handling different sparsity patterns] The sparsity pattern enters the code generator as either an MTX file or a \texttt{Matrix[bool]}, and immediately gets processed by a \texttt{Cursor}. The generation algorithm interacts with the sparsity pattern primarily by calling \mintinline{python}{cursor.has_nonzero_cell(...)}. On the other hand, if the generation algorithm needs a \emph{recursive} block decomposition, it is free to extract the sparsity pattern for a given block and wrap that in a new Cursor. 

        \item[Handling different block decompositions] The generation algorithm is free to iterate over m, n, and k blocks without having to explicitly consider their shape or number. It can also cleanly handle the case of non-constant block sizes, but this option must always be opt-in, as some generators, particularly TiledSparse, make regularity assumptions that this would violate. 

        \item[Handling different matrix formats] Various cursors currently support column-major dense, column-major sparse, CSC, CSR, BCSC, and BCSR matrix formats. Future work could readily extend this to support diagonal formats as well. It is also important here that the generators validate that they can handle the choice of format. 

        \item[Handling different memory addressing schemes] The choice of memory addressing scheme strongly affects instruction size, notably in the case of FMAs. A common optimization is to use scale-index addressing, which looks as follows:

         \begin{minted}{python}
         base_register + (scale_constant * index_register) + offset_constant
         \end{minted}

         The downside is that this involves using an unknown number of scalar registers and obfuscates the code. The generator itself is agnostic to the format of address given.

    \end{description}

Internally, the cursors all maintain an implicit or explicit table of offsets, mapping 
    The following cursor implementations play various roles 

    \begin{description}
        \item[DenseCursor] supports column-major and row-major dense matrix formats including padding. It is used to traverse every dense matrix. Under the hood, it is a facade around TiledCursor.

        \item[MiniCursor] handles the case of a sparse matrix which has not been divided into blocks. Under the hood, it is also a facade around TiledCursor.

        \item[TiledCursor] 

        \item[BlockCursor]
        \item[GeneralBlockCursor] Note: While this has not yet been implemented, it is technically straightforward.
    \end{description}

    * BCSR
    * SIBlockCursor
    * Irregular blocksize cursor. BlockedSparse and GeneralSparse



\section{Symbolic execution and reification}


In a very general sense, the GEMM generators implemented here all take information which was previously discovered at runtime and move it to compile time. Up until now, this activity has been denoted by the verb ``unroll''. However, this term is somewhat misleading. It comes from the concept of loop unrolling, a standard compiler optimization[ref to book]. Unrolling a loop is a path-independent transformation of an abstract syntax tree which has no access to new information. Unrolling a sparsity pattern is a path-dependent transformation of an (even more) abstract syntax tree which depends on the local program state space. A better word for this activity might be \emph{reification}. 

The pseudocode for UnrolledSparse and TiledSparse differ only by a single line, but their implementations differ substantially. This is because there is a deep asymmetry between how a looped block and an unrolled block interact with MatrixCursor. The critical question is ``Is a MatrixCursor stateful or not?'' An unrolled block which traverses a MatrixCursor needs to track the current CursorLocation one way or another. (Even a simple iteration variable is in effect simulating the state of the program's runtime.) On the other hand, a looped block knows nothing about

It is possible, but nontrivial and inelegant, to thread the MatrixCursor state 

Due to Python's eager evaluation of expressions, it is nontrivial and inelegant. An elegant solution does exist, and will be described below, even though it is beyond the scope of this work.


0. Weirdness surrounding unrolling, threading through machine state. 

1. AST->Symbolic execution

2. Concept of a command. Concept of a virtual machine outputting 

3. Recast Matrix, MatrixCursor, RegisterBlocks as a virtual machine

4. Computer algebra system as a virtual machine. Work already being done 

5. Formally verifying?

6. Design space of correct algorithms. Creating an 'optimal' GEMM instead of an 'optimized' one






One advantage to expressing a complete AST in Python


\chapter{Implementation}
\label{chapter:implementation}

This chapter covers the structure of the algorithm generators described previously and the resulting layout of the codebase. The first section discusses some key design decisions and the constraints driving them. The subsequent sections build up the machinery used for algorithm generation, moving from low-level to high-level. The foundation is a Python abstract syntax tree for assembly programs. These basic building blocks are aggregated and parameterized to create reusable components for subtasks such as manipulating register blocks. In order to manage the complexity of generating a traversal of a matrix with both a nontrivial sparsity pattern and a nontrivial memory layout, a \emph{matrix cursor} abstraction is introduced. This raises the possibility of a more general approach to moving information from runtime to compile time. The chapter ends with a description of the various entry points into the codebase and a brief usage guide.

\section{Design Decisions}

The first major decision was the choice of output language. There were three options available. The first option was to emit plain C/C++ code annotated with pragmas, as Breuer did. The second option was to emit C/C++ code using intrinsics. The final option was to emit plain assembly. Plain assembly was chosen for the reasons discussed below:

\begin{enumerate}

\item The primary consideration was reducing the layers of indirection. Many of the necessary design decisions are not directly expressible using C semantics. While there exist pragmas to control optimizations such as loop unrolling and vectorization, they do not always combine in predictable ways. One is forced to simultaneously reason at multiple abstraction levels: the C semantics, in order to obtain a correct program; the effect of different pragmas on code generation, in order to obtain the desired assembly; and finally the performance of the assembly on the actual hardware. Since the goal of this work is to explore how low-level algorithm changes affect performance on a specific architecture, the higher-level tools -- particularly, C with pragmas -- introduce considerable noise without introducing expressiveness to compensate.

\item The second consideration was the ease of gaining knowledge through experimentation. Given a C program which heavily uses intrinsics, an optimizing compiler will most likely produce a much more performant program compared to handwritten assembly. However, the extra optimization steps can obfuscate our experiment results, weakening our understanding of how the hardware reacts to small, precise changes. This understanding directly drives our algorithm design, which has a far greater potential upside than automatic optimization alone.

\item Another consideration was the principle of least power, as articulated by Tim Berners-Lee in~\cite{PrincipleOfLeastPower}. Generating a proper C abstract syntax tree would require considerable effort, and generating templated strings would be very fragile. Neither representation could be conveniently used by tools other than the compiler. On the other hand, an assembly AST would be easy to implement, and it could be gradually extended to support higher-level semantics. Small tools may then independently interact with this AST. Such tools may range from prettyprinters, to reordering transformations, to full interpreters. Separating these concerns leads to a cleaner and more reliable architecture.

\item The final consideration was continuity with existing codebases. The \py{libxsmm} library already emits plain assembly, and it would be straightforward to reimplement its microkernel using an assembly AST. Each successive investigation would build upon this basic codebase, leading to a lower-risk software engineering process.

\end{enumerate}

The second major design decision was the choice of language for the code generator itself. The two main requirements were that the language be amenable to rapid prototyping, be well-suited as a base for lightweight \glspl{DSL}, and have convenient access to numerical libraries. Two languages closely fit this niche: Python and Julia. Of the two, Python won due to its practicality -- the current SeisSol code generation framework is already written in Python, the libraries are mature, and the code can be run directly on the cluster. 

Julia, in comparison, has several notable advantages. Firstly, it has a much stronger type system, leading to both better performance and better correctness checking. Secondly, it is homoiconic, making it very amenable to metaprogramming. Thirdly, it is JIT-compiled on top of LLVM, which allows for features such as generating and displaying assembly code straight from the REPL. 

Ultimately Julia was considered too risky because the language and libraries are changing quickly. Furthermore, the benefit of being able to mix high-level and low-level programming paradigms was cancelled out by the lack of support on the cluster: when run locally, the Julia interpreter must not emit AVX-512 instructions. The prospect of implementing the ideas explored in this work as Julia macros remains intriguing, but also well out of scope.

Two downsides to using Python emerged over the course of the work. Firstly, refactoring proved to be surprisingly time-consuming, given the lack of type safety. This was partially mitigated by leveraging the new type annotation syntax along with the mypy type checker. Use of another new but irresistible syntax, `f'-string interpolation, constrained the code's compatibility to Python 3.6, which is not installed on the cluster either.

These considerations lead to a strategic decision to keep a clear distinction between code generation and runtime, thus avoiding real metaprogramming. The code generation framework is architecture independent, written in pure Python, and emitting output in plain text. This plain text file happens to be a C++ program containing inlined AVX-512 instructions, which must be uploaded to the cluster, compiled, and run there. The upside of this approach is that it is simple and it allows the user to manually tinker with the generated C++/assembly. The downside is that it substantially increases the time and effort it takes to obtain feedback after making changes. 




\section{Abstract Syntax Tree}

The most basic piece of machinery we need is a Python representation of the assembly code we emit. This is fundamentally an abstract syntax tree (AST) whether we recognize it or not, so we choose to recognize it. There is an inherent conflict between choosing data representations which are optimal for humans understanding the AST, optimal for custom tools modifying the AST, or optimal for producing textual assembly code. This is acerbated by Python's lack of constructor polymorphism and encapsulation. The human benefits from readability and a tower of abstractions which successively add specificity such as register length, precision, alignment, label names, and possibly even register names. The automated tools benefit from having a guarantee than any AST node is fully articulated, while still allowing select pieces to be abstract. Finally, the assembler needs every piece of information, repeated for every instruction, in a somewhat idiosyncratic format.

The solution to this conundrum is to decouple these representations. The AST is stripped down to be nothing more than a data container with a constructor that enforces some basic completeness invariants. The fields are chosen to be as \emph{complete} and \emph{orthogonal} as possible. The goals of being simple to work with, and of having a close mapping to the resulting assembly, are both disregarded with prejudice. Responsibility for the former is moved to the DSL, and responsibility for the latter is moved to the prettyprinter. This separation of concerns opens up long-term possibilities such as emitting C intrinsics instead of plain assembly, and in the short term it keeps the code legible.

The syntax tree is built up in stages of increasing complexity. All AST nodes inherit from the abstract base class \py{AsmStmt}, which enforces the existence of an \py{accept()} method, a \py{comment} string, and nothing else. 

\subsection{Operands}
Operands are the primitive `nouns' in this language. They consist of constants, labels, registers, and memory addresses. They can all be stringified into pseudocode or into GAS syntax. However, they are simpler than GAS's own concept of operands. For example, the FMA instruction expresses a broadcast operand like this: \verb|16(%%rsi)%{1to8%}|, whereas our language makes the operand be a plain memory address \py{(rsi, None, 1, 16)} and passes responsibility for realizing that this operand needs to be broadcasted on to the \py{fma()} instruction. The weird GAS syntax for achieving this exists only in the GAS prettyprinter.

\subsection{Instructions}

An instruction is an AST node which can be mapped into one or more assembly instructions. This work currently only requires seven different instructions, which are described below. 

\begin{description}
  \item [Move] Wraps \verb|mov|, \verb|vmovapd|, and \verb|vmovupd|, using type information to choose. 
  \item [Fma] Wraps \verb|vfmadd231pd|. Can optionally broadcast from a MemoryAddress. The choice of precision should eventually be determined from type information. Specifying mask registers will be needed as well.
  \item [Add] Wraps \verb|add| for now; should eventually handle the vectorized version as well. 
  \item [Compare] Compares two Operands, without getting confused over which is the left-hand side and which is the right.
  \item [Jump] Jumps to an Operand; jump is indirect if the Operand is not a Label. Also supports local labels with direction specifiers. Unlike GAS, the direction specifier is part of the Jump node, not part of the Label. Whether or not a label is represented as local is independent of the underlying Label itself.
  \item [DeclLabel] Declares a label.
  \item [DeclData] Pastes a \py{Label} or a \py{Constant} directly into the code segment, wrapping the \py{db,dw,dd,dq} instructions. It is useful for creating jump tables or constant arrays. It could be modified to target other segments as well.
\end{description}

\subsection{Blocks} 

Blocks are the first level of structure. A block is simply a list of AST nodes, along with a comment. Blocks are valuable because they make it possible to recover a stack trace \emph{through the code being generated} if the code generation fails. If the code generation succeeds, the prettyprinter indents based on block structure, making the code far more readable. 

A common pattern inside the code generators is to have a function which takes certain parameters and returns a Block. This is used to generate reusable components, such as the inner part of MicroSparse, or register block movements. This is only slightly better than a preprocessor, because there is nothing controlling that the AST nodes inside the block make any sense together. Such functions usually benefit from having assert statements to enforce preconditions necessary for the generation to succeed.

\subsection{Forms}

Forms are the next level of structure. Whereas a block behaves like a simple container, forms behave more like a macro. They accept as arguments one or more blocks, and then transform these arguments however they see fit. This is how JumpTables are implemented. They accept Blocks containing each of the MicroSparse kernels, and they arrange them around the necessary labels and jumps. While this could be done using just a function that returns a Block, doing so would throw away the higher-level information about what is really going on. By keeping this information, we are essentially adding new syntax to our AST.

\subsection{Commands}

Commands are experimental and don't currently work. Their purpose is to express higher-level ideas which require knowledge of the program's state at runtime. This should be contrasted with Forms, which manipulate the AST without any such knowledge. Commands allow information -- that was previously known only at runtime -- to be moved to compile time. The motivating example is the matrix sparsity pattern. When we wish to move a pointer to B to the next block, and the block size is irregular, \emph{and we are unrolling the loop}, we can do so. We can track the location of the pointer as each new instruction is emitted, and use the sparsity pattern to calculate the offset to the start of the next block. Commands generalize this idea, and shall be explored further in Section~\ref{section:theory}, once we have covered matrix cursors. 

\section{Interacting with an AST}

The mechanisms for interacting with an AST were deliberately separated from the data structure itself in order to enforce a clean separation of concerns. We end up with two main interfaces: a human-facing interface for assembling ASTs, and a machine-facing interface for traversing and modifying them. 

The human interface is pure syntactic sugar. All of the KNL registers and datatypes are defined as global variables along with functions which are defined as conveniently as possible for constructing different AST nodes. A `machine context' stores sensible defaults for choices like precision, alignment, and eventually perhaps vector length and available ISA extensions. An eventual goal is to make these swappable, so that the same generator may output AVX2 instead of AVX512 simply by loading a different module.

When it comes to organizing instructions into blocks, there are two options available. The first is a BlockBuilder class, which allows the programmer to add AST nodes to the current block, open a deeper block, or close the current block and move to its parent. This is particularly convenient when a loop is being manually unrolled. The second option is an interface inspired by S-expressions, where a block is merely a function call, and its contents the argument list. This is more convenient for laying out nested, static code such as non-unrolled loops.

The machine interface is a Visitor Pattern. A Visitor Pattern allows a method to be defined polymorphically for all AST nodes in an isolated, self-contained, extensible way while maintaining static type safety. In practice what this means is that the AST nodes all have an \py{accept(self, visitor)} method, and each new `method' is in reality a Visitor class which contains specialized methods \py{visitMov(), visitFma(), visitAdd(),} etc. Thus every function which might wish to traverse the AST lives in its own separate module.

Visitors currently exist for prettyprinting the AST into valid GAS syntax, prettyprinting into pseudocode, counting FMA instructions, and determining which registers have been used or modified. In the future they could be used for static type checking, instruction reordering, or abstract interpretation.

\section{Cursors}
\label{section:cursors}

    The \emph{matrix cursor} is an abstraction that allows the code generators to cleanly traverse a sparse matrix and access its contents while only using logical coordinates. The name was chosen because of its similarities to the cursor in a text editor, which must translate movement and insert commands from 2D screen coordinates into a 1D memory location which depends on the lengths on the neighboring lines. The term 'cursor' has already been overloaded to describe a kind of iterator into a database; this is also a kind of iterator. MatrixCursor performs the following tasks:

    \begin{description}

    \item[Traversing a matrix by blocks.] MatrixCursor translates logical block coordinates \texttt{(Bki, Bni)} into a movement instruction. This instruction updates a register pointing to the start of the `active' block. The logical block coordinates may be either absolute or relative to the active block.

    \item[Optionally tracking which block is active.] Generating a GEMM is much simpler if the generator can model the state of the machine while deciding which code to emit in order to modify said state. Not all generators implemented currently do this; some emit loops which do not get simulated, and then assume that the loops maintain certain invariants. This is the start of a more theoretical discussion in Section~\ref{section:theory}. 

    \item[Accessing cells within a block.] MatrixCursor translates logical cell coordinates \texttt{(bki, bni)} into a \texttt{MemoryAddress} operand. Cell coordinates are relative to the top-left-corner of the active block. It also generates a comment string (which can be rendered alongside the assembly) indicating which logical coordinates lie behind the otherwise inscrutable memory address.

    \item[Handling the case of empty blocks and cells.] The generators may directly query whether a given block or cell exists. If the generators attempt to move to an empty block or access an empty cell, the MatrixCursor will throw an exception because there is no meaningful memory address which can be associated with that logical location. Pre-emptively checking for these cases makes the generation code much cleaner.

    \item[Retrieving information about the active block.] This information includes the block shape \texttt{bk, bn}, sparsity pattern, and pattern index. This is inessential, but has proven convenient for implementing nested block decompositions.

    \end{description}

    This is achieved through the interface in Listing~\ref{lst:matrixcursor_interface}.
    \begin{listing}
        \begin{minted}{python}

            class Cursor:
                def has_nonzero_cell(self, 
                                     current_block: CursorLocation, 
                                     target_block: Coords, 
                                     target_cell: Coords) -> bool:
                    pass

                def has_nonzero_block(self, 
                                      current_block: CursorLocation, 
                                      dest_block: Coords) -> bool:
                    pass

                def move(self, 
                         current_block: CursorLocation, 
                         target_block: Coords) -> Tuple[AsmStmt, CursorLocation]:
                    pass

                def look(self, 
                         current_block: CursorLocation, 
                         target_block: Coords, 
                         target_cell: Coords) -> Tuple[MemoryAddress, str]:
                    pass

                def start_location(self, target_block: Coords) -> CursorLocation:
                    pass

                def get_block(self, current_block: CursorLocation, target_block: Coords) -> BlockInfo
                    pass
        \end{minted}
        \caption{MatrixCursor interface}
        \label{lst:matrixcursor_interface}
    \end{listing}

    The purpose of a matrix cursor is to separate concerns in order to keep the algorithm generation logic as clean and simple as possible. Otherwise there would be a proliferation of minor variations of the same few algorithms. The complexity which is encapsulated includes the following considerations:

    \begin{description}
        \item[Handling different sparsity patterns] The sparsity pattern enters the code generator as either an MTX file or a \texttt{Matrix[bool]}, and immediately gets processed by a \texttt{Cursor}. The generation algorithm interacts with the sparsity pattern primarily by calling \mintinline{python}{cursor.has_nonzero_cell(...)}. On the other hand, if the generation algorithm needs a \emph{recursive} block decomposition, it is free to extract the sparsity pattern for a given block and wrap that in a new Cursor. 

        \item[Handling different block decompositions] The generation algorithm is free to iterate over m, n, and k blocks without having to explicitly consider their shape or number. It can also cleanly handle the case of non-constant block sizes, but this option must always be opt-in, as some generators, particularly TiledSparse, make regularity assumptions that this would violate. 

        \item[Handling different matrix formats] Various cursors currently support column-major dense, column-major sparse, CSC, CSR, BCSC, and BCSR matrix formats. Future work could readily extend this to support diagonal formats as well. It is also important here that the generators validate that they can handle the choice of format. 

        \item[Handling different memory addressing schemes] The choice of memory addressing scheme strongly affects instruction size, notably in the case of FMAs. Scale-index addressing is preferred, but this requires maintaining a collection of registers whose sole purpose is to be part of the memory address,
        and then choosing a combination of base and index registers such that the offset stays under 128. 

    \end{description}

    The following cursor implementations currently exist. A future cursor could support variable block sizes.

    \begin{description}
        \item[TiledCursor] assumes a single sparsity pattern which tiles perfectly. 

        \item[DenseCursor] supports column-major and row-major dense matrix formats including padding. It is used to traverse every dense matrix. Under the hood, it is a facade around TiledCursor.

        \item[MiniCursor] handles the case of a sparse matrix which has not been divided into blocks. Under the hood, it is also a facade around TiledCursor.

        \item[BlockCursor] handles the case 

        \item[GeneralBlockCursor] Note: While this has not yet been implemented, it is technically straightforward.
    \end{description}


\section{Symbolic execution and reification}
\label{section:theory}


In a very general sense, the GEMM generators implemented here all take information which was previously discovered at runtime and move it to compile time. Up until now, this activity has been denoted by the verb ``unroll''. However, this term is somewhat misleading. It comes from the concept of loop unrolling, a standard compiler optimization. Unrolling a loop is a path-independent transformation of an abstract syntax tree which has no access to new information. Unrolling a sparsity pattern is a path-dependent transformation of an (even more) abstract syntax tree which depends on the local program state space. A better word for this activity might be \emph{reification}. 

The pseudocode for UnrolledSparse and TiledSparse differ only by a single line, but their implementations differ substantially. This is because there is a deep asymmetry between how a looped block and an unrolled block interact with MatrixCursor. The critical question is ``Is a MatrixCursor stateful or not?'' An unrolled block which traverses a MatrixCursor needs to track the current CursorLocation one way or another. (Even a simple iteration variable is in effect simulating the state of the program's runtime.) On the other hand, a looped block knows nothing about this, and instead must rely on assumed invariants, such as a constant, perfectly tiled sparsity pattern.

In order to unify the concepts of a loop and an ``unrolled'' (reified) loop, so that we may automatically generate assembly code for both using the same tools, it is necessary to thread the machine state consistently throughout the code generation. This is effectively a symbolic execution of the code. 

Because the AST can be extended to support higher-level concepts using Forms and Commands, this symbolic execution need not only concern itself with transformations of individual registers and memory addresses. It can work at an arbitrary abstraction level. In fact, pieces of a high-level interpreter already exist: the Matrix, RegisterBlock, and MatrixCursor classes all work together to capture most of the matrix-multiplication semantics. If the methods on these classes were recasted as Commands, we would end up with a high-level imperative language specifically for matrix multiplication. Executing this language on the high-level interpreter would effectively translate the commands down to lower-level assembly code. 


\section{User Manual}



\subsection{sparsemmgen}

\texttt{sparsemmgen} is the primary entry point for the program. It requires a minimal set of parameters: the choice of algorithm generator, values of $m, n, k, lda, ldb, ldc,$ and the path to an MTX file containing the sparsity pattern. It emits a C function containing inline assembly straight to standard out. For closer control over the generators it accepts optional parameters: values of $bm, bn, bk$, the output filename, the output function name, and output format. The latter allows the user to choose between GCC, GAS, or pseudocode.

If the user does not provide the optional parameters, the generator will attempt to choose values automatically based off of heuristics derived via experiment and a little bit of brute force. 


\subsection{libxsmmproxy}

While \py{sparsemmgen} is designed to allow the user to control the algorithm generator precisely, it suffers from two drawbacks. Firstly, the burden of choosing the best algorithm and tuning it correctly is put entirely on the user. Secondly, if an existing codebase has been sufficiently optimized such that its developers would consider using the tools developed here, the integration would likely be very difficult, since the information \py{sparsemmgen} needs is unlikely to be present. Reducing the barrier to integration increases the likelihood of this tool being used.

\py{libxsmmproxy} addresses both of these points. It is a command-line program which matches the interface of \py{libxsmm_gemm_generator} exactly and can be integrated into an existing codebase by changing a path or adding a symbolic link. (No plans for creating a JIT version exist at this time.) The program analyzes its arguments and decides whether to use the \py{sparsemmgen} implementation or to simply delegate to the original \py{libxsmm_gemm_generator}.

Unfortunately, the information which the \py{libxsmm_gemm_generator} interface provides only partially overlaps with the information which \py{sparsemmgen} needs. On one hand, \py{libxsmm_gemm_generator} does not know about our generators or their parameters such as block sizes. On the other, it supports different microarchitectures, single precision, and different prefetching strategies, which this work currently does not. Thus the proxy must pattern-match for the KNL architecture, double precision, and no prefetching strategy, and delegate all other requests. Then it must conjure the generator choice and its parameters.

There are two approaches to supplying the missing information, one manual and one automatic. The manual approach is a whitelist, in pure Python, which maps specific MTX files to \py{Parameters} objects. It is advantageous insofar as it gives the user precise control, and disadvantageous insofar as it is repetitive and requires knowledge of the system. It is best combined with the automatic system described next.

In theory, automatically choosing the best generator involves determining, for each generator:
\begin{itemize}
  \item{Can the generator handle the posed problem at all?}
  \item{What is the best choice of parameters for this (problem, generator)?}
  \item{What is the expected performance of this choice?}
\end{itemize}
and then choosing the (generator, parameters) which offer the best performance. In the general case this is a combinatorial optimization problem, but in practice the matrix sizes are very small and the parameter space is very constrained, particularly when only allowing perfect tilings. Thus a brute-force approach is satisfactory for the time being. 

In the case of dense-by-sparse multiplication, a brute-force approach is not even necessary most of the time. If the number of nonzeros in the B matrix is below the limit imposed by the instruction cache, there is no reason to use any generator besides UnrolledSparse. If it is greater, UnrolledSparse is out of the question, and the controller need only choose between TiledSparse, BlockedSparse, GeneralSparse, and dense. TiledSparse and BlockedSparse are likely to perform very well on certain sparsity patterns but not in the general case, so the automatic system can default to using GeneralSparse. There will be a point at which the penalty of performing indirect jumps outweighs the saved FLOPs, in which point the system should simply use the dense algorithm. This we can determine experimentally.


\subsection{runexperiment}

The experiments discussed in the next chapter can all be reproduced by calling \py{runexperiment}. The experiments rely on generated data being in the correct directory, so it is best to clone the repository on the cluster and run it from there. A Python wrapper for interacting with SLURM is provided, and it supports remoting over SSH. If the SLURM setup breaks down, there are also recipes in the Makefile for building and running each experiment once the C++ code has been generated.







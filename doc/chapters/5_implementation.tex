
\chapter{Implementation}
\label{chapter:implementation}

This chapter covers the decomposition of the algorithm generators described previously and the resulting layout of the codebase. The first section discusses some key design decisions and the constraints driving them. The second section describes the various entry points into the codebase and doubles as a brief usage guide. The subsequent sections build up the machinery used for algorithm generation, moving from low-level to high-level. The foundation is a Python abstract syntax tree for assembly programs. These basic building blocks are aggregated and parameterized to create reusable components for subtasks such as manipulating register blocks. In order to manage the complexity of generating a traversal of a matrix with both a nontrivial sparsity pattern and a nontrivial memory layout, a \emph{matrix cursor} abstraction is introduced. This raises the possibility of a more general approach to moving information from runtime to compile time. The chapter ends with a summary of each software module's purpose and dataflow.

\section{Design Drivers}

- Choice of assembly
- Choice of Python


\section{Frontends}

\subsection{sparsemmgen}

\texttt{sparsemmgen} is the primary entry point for the program 

\subsection{libxsmmproxy}

While \texttt{sparsemmgen} is designed to allow the user to control the algorithm generator precisely, it suffers from two drawbacks. Firstly, the burden of choosing the best algorithm and tuning it correctly is put entirely on the user. As the following chapter will show, performance heuristics can be derived and verified experimentally. This allows the program to automatically determine the best parameters for each algorithm, and ultimately determine the choice of algorithm as well. Secondly, if an existing codebase has been sufficiently optimized such that its developers would consider using the tools developed here, the integration would likely be very difficult. \texttt{sparsemmgen} requires information which is simply not likely to be available upstream. On the other hand, codebases that could benefit from this work are likely already using \texttt{libxsmm}. 

\texttt{libxsmmproxy} 

\begin{itemize}
	\item Auto-choosing parameters
	\item Hardcoding parameters
\end{itemize}

\subsection{runexperiment}
The third main entry point is \texttt{runexperiment}, which generates a 



\section{Code generation}
\begin{itemize}
	\item Choice of language/patterns
	\item Inspiration from older languages
    \item AST

\end{itemize}

\begin{itemize}
    \item Python domain-specific language for directly generating assembly
    \item Includes rich assembly AST: \texttt{Operand} $\leftarrow$ \texttt{Statement} $\leftarrow$ \texttt{Block} 
    \item Higher-level forms such as loops, subroutines, and jump tables
          implemented as plain Python functions which return a \texttt{Block}
    \item Pretty-printing, analysis, simulation are implemented using Visitor Pattern
    \item Interesting theoretical problem: Moving information from runtime to compile time in a general way
\end{itemize}

\section{Components}
\subsection{Register blocks}
\subsection{Microkernels}

\section{Cursors}


    The \emph{matrix cursor} is an abstraction that allows the code generators to cleanly traverse a sparse matrix and access its contents while only using logical coordinates. The name was chosen because of its similarities to the cursor in a text editor, which must translate movement and insert commands from 2D screen coordinates into a 1D memory location which depends on the lengths on the neighboring lines. The term 'cursor' has already been overloaded to describe a kind of iterator into a database; this is also a kind of iterator. MatrixCursor performs the following tasks:

    \begin{description}

    \item[Traversing a matrix by blocks.] MatrixCursor translates logical block coordinates \texttt{(Bki, Bni)} into a movement instruction. This instruction updates a register pointing to the start of the `active' block. The logical block coordinates may be either absolute or relative to the active block.

    \item[Optionally tracking which block is active.] Generating a GEMM is much simpler if the generator can model the state of the machine while deciding which code to emit in order to modify said state. Not all generators implemented currently do this; some emit loops which do not get simulated, and then assume that the loops maintain certain invariants. This is the start of a more theoretical discussion in Section X.XX. 

    \item[Accessing cells within a block.] MatrixCursor translates logical cell coordinates \texttt{(bki, bni)} into a \texttt{MemoryAddress} operand. Cell coordinates are relative to the top-left-corner of the active block. It also generates a comment string (which can be rendered alongside the assembly) indicating which logical coordinates lie behind the otherwise inscrutable memory address.

    \item[Handling the case of empty blocks and cells.] The generators may directly query whether a given block or cell exists. If the generators attempt to move to an empty block or access an empty cell, the MatrixCursor will throw an exception because there is no meaningful memory address which can be associated with that logical location. Pre-emptively checking for these cases makes the generation code much cleaner.

    \item[Retrieving information about the active block.] This information includes the block shape \texttt{bk, bn}, sparsity pattern, and pattern index. This is inessential, but has proven convenient for implementing nested block decompositions.

    \end{description}

    This is achieved through the following interface:
    \begin{listing}
        \begin{minted}{python}

            class Cursor:
                def has_nonzero_cell(self, 
                                     current_block: CursorLocation, 
                                     target_block: Coords, 
                                     target_cell: Coords) -> bool:
                    pass

                def has_nonzero_block(self, 
                                      current_block: CursorLocation, 
                                      dest_block: Coords) -> bool:
                    pass

                def move(self, 
                         current_block: CursorLocation, 
                         target_block: Coords) -> Tuple[AsmStmt, CursorLocation]:
                    pass

                def look(self, 
                         current_block: CursorLocation, 
                         target_block: Coords, 
                         target_cell: Coords) -> Tuple[MemoryAddress, str]:
                    pass

                def start_location(self, target_block: Coords) -> CursorLocation:
                    pass

                def get_block(self, current_block: CursorLocation, target_block: Coords) -> BlockInfo
                    pass
        \end{minted}
        \caption{MatrixCursor interface}
        \label{lst:matrixcursor_interface}
    \end{listing}

    The purpose of a matrix cursor is to separate concerns in order to keep the algorithm generation logic as clean and simple as possible. Otherwise there would be a proliferation of minor variations of the same few algorithms. The complexity which is encapsulated includes the following considerations:

    \begin{description}
        \item[Handling different sparsity patterns] The sparsity pattern enters the code generator as either an MTX file or a \texttt{Matrix[bool]}, and immediately gets processed by a \texttt{Cursor}. The generation algorithm interacts with the sparsity pattern primarily by calling \mintinline{python}{cursor.has_nonzero_cell(...)}. On the other hand, if the generation algorithm needs a \emph{recursive} block decomposition, it is free to extract the sparsity pattern for a given block and wrap that in a new Cursor. 

        \item[Handling different block decompositions] The generation algorithm is free to iterate over m, n, and k blocks without having to explicitly consider their shape or number. It can also cleanly handle the case of non-constant block sizes, but this option must always be opt-in, as some generators, particularly TiledSparse, make regularity assumptions that this would violate. 

        \item[Handling different matrix formats] Various cursors currently support column-major dense, column-major sparse, CSC, CSR, BCSC, and BCSR matrix formats. Future work could readily extend this to support diagonal formats as well. It is also important here that the generators validate that they can handle the choice of format. 

        \item[Handling different memory addressing schemes] The choice of memory addressing scheme strongly affects instruction size, notably in the case of FMAs. A common optimization is to use scale-index addressing, which looks as follows:

         \begin{minted}{python}
         base_register + (scale_constant * index_register) + offset_constant
         \end{minted}

         The downside is that this involves using an unknown number of scalar registers and obfuscates the code. The generator itself is agnostic to the format of address given.

    \end{description}

Internally, the cursors all maintain an implicit or explicit table of offsets, mapping 
    The following cursor implementations play various roles 

    \begin{description}
        \item[DenseCursor] supports column-major and row-major dense matrix formats including padding. It is used to traverse every dense matrix. Under the hood, it is a facade around TiledCursor.

        \item[MiniCursor] handles the case of a sparse matrix which has not been divided into blocks. Under the hood, it is also a facade around TiledCursor.

        \item[TiledCursor] 

        \item[BlockCursor]
        \item[GeneralBlockCursor] Note: While this has not yet been implemented, it is technically straightforward.
    \end{description}

    * BCSR
    * SIBlockCursor
    * Irregular blocksize cursor. BlockedSparse and GeneralSparse

\section{Symbolic execution and reification}

One advantage to expressing a complete AST in Python


\chapter{Implementation}
\label{chapter:implementation}

This chapter covers the decomposition of the algorithm generators described previously and the resulting layout of the codebase. The first section discusses some key design decisions and the constraints driving them. The second section describes the various entry points into the codebase and doubles as a brief usage guide. The subsequent sections build up the machinery used for algorithm generation, moving from low-level to high-level. The foundation is a Python abstract syntax tree for assembly programs. These basic building blocks are aggregated and parameterized to create reusable components for subtasks such as manipulating register blocks. In order to manage the complexity of generating a traversal of a matrix with both a nontrivial sparsity pattern and a nontrivial memory layout, a \emph{matrix cursor} abstraction is introduced. This raises the possibility of a more general approach to moving information from runtime to compile time. The chapter ends with a summary of each software module's purpose and dataflow.

\section{Design Drivers}

- Choice of assembly
- Choice of Python


\section{Frontends}

\subsection{sparsemmgen}

\texttt{sparsemmgen} is the primary entry point for the program 

\subsection{libxsmmproxy}

While \py{sparsemmgen} is designed to allow the user to control the algorithm generator precisely, it suffers from two drawbacks. Firstly, the burden of choosing the best algorithm and tuning it correctly is put entirely on the user. Secondly, if an existing codebase has been sufficiently optimized such that its developers would consider using the tools developed here, the integration would likely be very difficult, since the information \py{sparsemmgen} needs is unlikely to be present. Reducing the barrier to integration increases the likelihood of this tool being used.

\py{libxsmmproxy} addresses both of these points. It is a command-line program which matches the interface of \py{libxsmm_gemm_generator} exactly and can be integrated into an existing codebase by changing a path or adding a symbolic link. (No plans for creating a JIT version exist at this time.) The program analyzes its arguments and decides whether to use the \py{sparsemmgen} implementation or to simply delegate to the original \py{libxsmm_gemm_generator}.

Unfortunately, the information which the \py{libxsmm_gemm_generator} interface provides only partially overlaps with the information which \py{sparsemmgen} needs. On one hand, \py{libxsmm_gemm_generator} does not know about our generators or their parameters such as block sizes. On the other, it supports different microarchitectures, single precision, and different prefetching strategies, which this work currently does not. Thus the proxy must pattern-match for the KNL architecture, double precision, and no prefetching strategy, and delegate all other requests. Then it must conjure the generator choice and its parameters.

There are two approaches to supplying the missing information, one manual and one automatic. The manual approach is a whitelist, in pure Python, which maps specific MTX files to \py{Parameters} objects. It is advantageous insofar as it gives the user precise control, and disadvantageous insofar as it is repetitive and requires knowledge of the system. It is best combined with the automatic system described next.

In theory, automatically choosing the best generator involves determining, for each generator:
\begin{itemize}
  \item{Can the generator handle the posed problem at all?}
  \item{What is the best choice of parameters for this (problem, generator)?}
  \item{What is the expected performance of this choice?}
\end{itemize}
and then choosing the (generator, parameters) which offer the best performance. In the general case this is a combinatorial optimization problem, but in practice the matrix sizes are very small and the parameter space is very constrained, particularly when only allowing perfect tilings. Thus a brute-force approach would be satisfactory for the time being. 

In the case of dense-by-sparse multiplication, a brute-force approach is not even necessary most of the time. If the number of nonzeros in the B matrix is less than the limit imposed by the instruction cache, there is no reason to use any generator besides UnrolledSparse. If it is greater, UnrolledSparse is out of the question, and the controller need only choose between TiledSparse, BlockedSparse, GeneralSparse, and dense. TiledSparse and BlockedSparse are likely to perform very well on certain sparsity patterns but not in the general case, so the automatic system can default to using GeneralSparse. There will be a point at which the penalty of performing indirect jumps outweighs the saved FLOPs, in which point the system should simply use the dense algorithm. This we can determine experimentally.


\subsection{runexperiment}
The third main entry point is \texttt{runexperiment}, which generates a 




\section{Code generation}



\begin{itemize}
	\item Choice of language/patterns
	\item Inspiration from older languages
    \item AST

\end{itemize}

\begin{itemize}
    \item Python domain-specific language for directly generating assembly
    \item Includes rich assembly AST: \texttt{Operand} $\leftarrow$ \texttt{Statement} $\leftarrow$ \texttt{Block} 
    \item Higher-level forms such as loops, subroutines, and jump tables
          implemented as plain Python functions which return a \texttt{Block}
    \item Pretty-printing, analysis, simulation are implemented using Visitor Pattern
    \item Interesting theoretical problem: Moving information from runtime to compile time in a general way
\end{itemize}

\section{Components}
\subsection{Register blocks}
\subsection{Microkernels}

\section{Cursors}


    The \emph{matrix cursor} is an abstraction that allows the code generators to cleanly traverse a sparse matrix and access its contents while only using logical coordinates. The name was chosen because of its similarities to the cursor in a text editor, which must translate movement and insert commands from 2D screen coordinates into a 1D memory location which depends on the lengths on the neighboring lines. The term 'cursor' has already been overloaded to describe a kind of iterator into a database; this is also a kind of iterator. MatrixCursor performs the following tasks:

    \begin{description}

    \item[Traversing a matrix by blocks.] MatrixCursor translates logical block coordinates \texttt{(Bki, Bni)} into a movement instruction. This instruction updates a register pointing to the start of the `active' block. The logical block coordinates may be either absolute or relative to the active block.

    \item[Optionally tracking which block is active.] Generating a GEMM is much simpler if the generator can model the state of the machine while deciding which code to emit in order to modify said state. Not all generators implemented currently do this; some emit loops which do not get simulated, and then assume that the loops maintain certain invariants. This is the start of a more theoretical discussion in Section X.XX. 

    \item[Accessing cells within a block.] MatrixCursor translates logical cell coordinates \texttt{(bki, bni)} into a \texttt{MemoryAddress} operand. Cell coordinates are relative to the top-left-corner of the active block. It also generates a comment string (which can be rendered alongside the assembly) indicating which logical coordinates lie behind the otherwise inscrutable memory address.

    \item[Handling the case of empty blocks and cells.] The generators may directly query whether a given block or cell exists. If the generators attempt to move to an empty block or access an empty cell, the MatrixCursor will throw an exception because there is no meaningful memory address which can be associated with that logical location. Pre-emptively checking for these cases makes the generation code much cleaner.

    \item[Retrieving information about the active block.] This information includes the block shape \texttt{bk, bn}, sparsity pattern, and pattern index. This is inessential, but has proven convenient for implementing nested block decompositions.

    \end{description}

    This is achieved through the following interface:
    \begin{listing}
        \begin{minted}{python}

            class Cursor:
                def has_nonzero_cell(self, 
                                     current_block: CursorLocation, 
                                     target_block: Coords, 
                                     target_cell: Coords) -> bool:
                    pass

                def has_nonzero_block(self, 
                                      current_block: CursorLocation, 
                                      dest_block: Coords) -> bool:
                    pass

                def move(self, 
                         current_block: CursorLocation, 
                         target_block: Coords) -> Tuple[AsmStmt, CursorLocation]:
                    pass

                def look(self, 
                         current_block: CursorLocation, 
                         target_block: Coords, 
                         target_cell: Coords) -> Tuple[MemoryAddress, str]:
                    pass

                def start_location(self, target_block: Coords) -> CursorLocation:
                    pass

                def get_block(self, current_block: CursorLocation, target_block: Coords) -> BlockInfo
                    pass
        \end{minted}
        \caption{MatrixCursor interface}
        \label{lst:matrixcursor_interface}
    \end{listing}

    The purpose of a matrix cursor is to separate concerns in order to keep the algorithm generation logic as clean and simple as possible. Otherwise there would be a proliferation of minor variations of the same few algorithms. The complexity which is encapsulated includes the following considerations:

    \begin{description}
        \item[Handling different sparsity patterns] The sparsity pattern enters the code generator as either an MTX file or a \texttt{Matrix[bool]}, and immediately gets processed by a \texttt{Cursor}. The generation algorithm interacts with the sparsity pattern primarily by calling \mintinline{python}{cursor.has_nonzero_cell(...)}. On the other hand, if the generation algorithm needs a \emph{recursive} block decomposition, it is free to extract the sparsity pattern for a given block and wrap that in a new Cursor. 

        \item[Handling different block decompositions] The generation algorithm is free to iterate over m, n, and k blocks without having to explicitly consider their shape or number. It can also cleanly handle the case of non-constant block sizes, but this option must always be opt-in, as some generators, particularly TiledSparse, make regularity assumptions that this would violate. 

        \item[Handling different matrix formats] Various cursors currently support column-major dense, column-major sparse, CSC, CSR, BCSC, and BCSR matrix formats. Future work could readily extend this to support diagonal formats as well. It is also important here that the generators validate that they can handle the choice of format. 

        \item[Handling different memory addressing schemes] The choice of memory addressing scheme strongly affects instruction size, notably in the case of FMAs. A common optimization is to use scale-index addressing, which looks as follows:

         \begin{minted}{python}
         base_register + (scale_constant * index_register) + offset_constant
         \end{minted}

         The downside is that this involves using an unknown number of scalar registers and obfuscates the code. The generator itself is agnostic to the format of address given.

    \end{description}

Internally, the cursors all maintain an implicit or explicit table of offsets, mapping 
    The following cursor implementations play various roles 

    \begin{description}
        \item[DenseCursor] supports column-major and row-major dense matrix formats including padding. It is used to traverse every dense matrix. Under the hood, it is a facade around TiledCursor.

        \item[MiniCursor] handles the case of a sparse matrix which has not been divided into blocks. Under the hood, it is also a facade around TiledCursor.

        \item[TiledCursor] 

        \item[BlockCursor]
        \item[GeneralBlockCursor] Note: While this has not yet been implemented, it is technically straightforward.
    \end{description}

    * BCSR
    * SIBlockCursor
    * Irregular blocksize cursor. BlockedSparse and GeneralSparse



\section{Symbolic execution and reification}


In a very general sense, the GEMM generators implemented here all take information which was previously discovered at runtime and move it to compile time. Up until now, this activity has been denoted by the verb ``unroll''. However, this term is somewhat misleading. It comes from the concept of loop unrolling, a standard compiler optimization[ref to book]. Unrolling a loop is a path-independent transformation of an abstract syntax tree which has no access to new information. Unrolling a sparsity pattern is a path-dependent transformation of an (even more) abstract syntax tree which depends on the local program state space. A better word for this activity might be \emph{reification}. 

The pseudocode for UnrolledSparse and TiledSparse differ only by a single line, but their implementations differ substantially. This is because there is a deep asymmetry between how a looped block and an unrolled block interact with MatrixCursor. The critical question is ``Is a MatrixCursor stateful or not?'' An unrolled block which traverses a MatrixCursor needs to track the current CursorLocation one way or another. (Even a simple iteration variable is in effect simulating the state of the program's runtime.) On the other hand, a looped block knows nothing about

It is possible, but nontrivial and inelegant, to thread the MatrixCursor state 

Due to Python's eager evaluation of expressions, it is nontrivial and inelegant. An elegant solution does exist, and will be described below, even though it is beyond the scope of this work.


0. Weirdness surrounding unrolling, threading through machine state. 

1. AST->Symbolic execution

2. Concept of a command. Concept of a virtual machine outputting 

3. Recast Matrix, MatrixCursor, RegisterBlocks as a virtual machine

4. Computer algebra system as a virtual machine. Work already being done 

5. Formally verifying?

6. Design space of correct algorithms. Creating an 'optimal' GEMM instead of an 'optimized' one






One advantage to expressing a complete AST in Python

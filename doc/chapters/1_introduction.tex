% !TEX root = ../main.tex
\chapter{Introduction}
\label{chapter:Introduction}


\section{Motivation}

\begin{description}
	\item[SeisSol]
    Simulates earthquakes, specifically, dynamic rupture processes and seismic wave propagation over an unstructured tetrahedral mesh. 

    \item[ADER-DG]
    A numerical method which uses Discontinuous Galerkin discretization in space and ADER discretization in time. It avoids a global system matrix, using local element matrix operations instead.

    \item[Small Sparse Matrix Multiplications]
    The compute kernels resemble the operation shown below, applied recursively. Different sparsity patterns are used, but they are all known at compile time.
\end{description}

\begin{figure}
  \centering
  \includegraphics[height=3cm]{images/seissol_visc.png}
  \caption{Common sparsity patterns in SeisSol}
  \label{fig:seissol_star}
\end{figure}

\section{Hardware Constraints}

Although the motivation is straightforward, it is growing increasingly difficult to implement in practice because it is working against current trends in computer architecture. An growing number of the top500 computers are using GPUs or Xeon Phis. This trend is likely to continue because it allows greater parallelism at a lower energy cost. It was desired to run the code on LRZ's CoolMUC3 cluster, which uses Knights Landing.

Knights Landing (KNL) is the second-generation Intel Xeon Phi, a manycore processor that offers greater parallelism and less power consumption in exchange for a slower clock speed. An overview is given by~\cite{Sodani:2016:KLS:2927511.2927563} and a more comprehensive resource is given by~\cite{Jeffers:2016:IXP:3050856}. Unlike its predecessor, Knights Corner, KNL is a host processor rather than a coprocessor, which means that it can run the entire x86-64 instruction set, albeit with a performance penalty for some operations. KNL has two kinds of memory, a high-bandwidth MCDRAM and a high-capacity DDR4. Each processor contains \~64 cores with 4 hyperthreads per core, resulting in 256 logical CPUs. The instruction-level parallelism is similarly impressive, being the first processor to implement the AVX-512 instruction set extensions. These provide 32 vector registers, each of which holds 64 bytes, allowing 8 double-precision floating point numbers to be operated on simultaneously in each vector processing unit (VPU). Instruction-level performance is further enhanced by an enhanced fused-multiply-add (FMA) instruction, which performs $c := c + a*b$ in a single cycle, optionally including a broadcast and a mask. Thus the theoretical peak performance of a single core is given as:

\begin{equation}
  2 \frac{VPUs}{core} \times  = 3 {double TFlop}{sec}
  \label{eq:knl_peak_perf}
\end{equation}

\[\]

This peak performance is in practice merely an upper bound, as it assumes a steady-state throughput of 2 FMAs per cycle. The only algorithms which come close to achieving this are dense matrix multiplication and (to a lesser extent) LU and Cholesky factorization. If the algorithm is not able to be vectorized at all, the attainable performance drops by a factor of 8, and if it cannot use the FMA instructions, it drops by a further factor of 2. This fundamentally limits 

\begin{itemize}
    \item Scalar operations limited to 1/8 peak performance
    \item Bottleneck with vector stores: 64B/cycle
    \item Bottleneck with instruction cache, decoder: min(16B/cycle, 2 instructions)
    \item \textbf{Out-of-order execution pipeline can only issue 2 instructions per cycle}
\end{itemize}

\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth]{images/knl_arch.pdf}
\caption{Block diagram of the Knights Landing architecture, taken from~\cite{Sodani:2016:KLS:2927511.2927563}. As shown in (a), each processor is divided into 36 tiles connected in a 2D mesh. As shown in (b), each tile contains 2 cores and a shared L2 cache. Each core has two VPUs and its own L1 caches.}
\end{figure}

\section{Previous approaches}

\subsection{CSC sparse multiplication}
  The first approach used standard compressed-sparse-column multiplication. This algorithm goes back to WHOM? . It looks as follows:


  It has the following advantages: It is extremely general, placing no constraints on the matrix size or sparsity. 
  The sparsity pattern itself is known only at runtime. This generality makes it extremely amenable to abstraction, so a 

  Nevertheless, the standard csc multiplication algorithm suffers from several crucial disadvantages: 

\begin{description}
  \item[High memory usage]

  \item[Computational intensity]

  \item[Inability to unroll the innermost loop] This hurts pipeline efficiency.

  \item[Scalar operations]

  The indices are looked up at runtime. Firstly, this requires storing the indices, doubling the amount of memory needed. If only one matrix is sparse, the total number of flops is $2 * m * n * nnz$, with nnz=1/4*n*k, this means that the arithmetic intensity [FLOPs/byte] is 1/8 that of dense matrix multiplication. 
  Although the format lays everything out in memory as efficiently, the algorithm must nevertheless loop over k. The resulting control dependency prevents . The fact that the sparsity pattern is only known at compile time prevents unrolling. 
  A dense-by-sparse algorithm would still benefit from vectorization, but if the 

\end{description}


In the case of SeisSol, the sparsity patterns are derived from the basis functions, which themselves depend mainly on order of accuracy and the choice of viscous damping model. These parameters are all chosen at compile time. Secondly, the problem size is always small: the largest dimension is either 34 (corresponding to ) 56. 

In the case of Knights Landing, these problems are all acerbated. 





  Standard compressed-sparse-column multiplication
    \begin{itemize}
      \item[$+$] Generality.
      \item[$+$] Algorithm uses a (small) constant number of instructions.
      \item[$-$] Index lookups require indirect memory access, causing lots of problems for the execution pipeline.
      \item[$-$] $sparse \times sparse$ is fundamentally scalar. (However, BSR can be vectorized.)
      \item[$-$] Disregards the fact that the sparsity pattern is known at compile time.
    \end{itemize}

  Sparsity pattern unrolled into instruction stream
    \begin{itemize}
    \item[$+$] Index lookups removed completely
    \item[$+$] $dense \times sparse$ case can be vectorized perfectly 
    \item[$-$] Generates one FMA instruction for every nonzero
    \item[$-$] (Implementation) Memory access pattern is suboptimal on KNL
    \item[$-$] (Implementation) Intel compiler struggles to generate efficient assembly
    \end{itemize}

  Current approach: Dense matrix multiplication via libxsmm
    \begin{itemize}
    \item[$+$] Memory access patterns and register blockings optimized for KNL
    \item[$+$] (Implementation) Code generator emits assembly directly 
    \item[$-$] Filling in zero entries wastes both flops and memory bandwidth
    \end{itemize}


  \section{Goals and approach}
  My approach
    \begin{itemize}
    \item The three existing algorithms each encounter different constraints and bottlenecks.
    \item Think of them as poles in a design space. Explore algorithms which combine their features in order to finesse around the bottlenecks.
    \item Speed up applications such as SeisSol, or demonstrate why it can't be done.
    \item Create a code generator which autotunes a sparse gemm algorithm for a reasonably arbitrary problem.
    \end{itemize}

 Goals
   - Explore the parameter space of different families of algorithms:
     + dense-sparse csc, tiledcsc, blockcsc
     + sparse-dense breuer, blended, shifted-load, scalar, gather-scatter

   - Independent variables include matrix size, overall sparsity, sparsity patterns, block size, register usage, loop unroll depth, instruction reordering

   - Identify bottlenecks and understand how they constrain the regions of parameter space in which they are useful

   
    \section{Remainder of paper}


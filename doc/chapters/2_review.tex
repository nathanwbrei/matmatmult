\chapter{Background}
\label{chapter:review}

\section{Matrix multiplication}

Dense and spare matrix multiplication together account for two of the "seven pillars" of computation, in the sense that they encompass an unexpectedly large fraction of all computing, particularly in scientific and engineering fields. Since any algorithmic improvement has the potential to improve the performance of programs across the board, these algorithms have been highly optimized for each successive computer architecture and made available through a common interface known as BLAS since 1977. However, it wasn't until 2008 that a more general approach for designing optimized dense matrix multiplication algorithms -- independent of the exact details of the memory hierarchy -- was established. This work by Goto takes advantage of the recursive nature of block-wise matrix multiplication in order to successively divide the problem into blocks, each of which is sized and padded to make best use of the memory hierarchy. Thus a very large matrix may be split among separate machines using Cannon's algorithm, then further split into blocks which fit in the L2 cache, then in the L1 cache, then finally in the registers themselves. 



What is BLAS?

What conventions does BLAS give us?

What did Goto figure out?

Why doesn't Goto's work apply here?

What is the outer-product formulation?

What does Libxsmm do differently from BLAS/Goto?

What about memory-addressing schemes

What about Breuer's work? Does it vectorize?

Sparse matrix formats BSR vs BCSR

What is the sparse/dense asymmetry?

What naming conventions do we use?



\begin{minted}{python}

C := alpha*op( A )*op( B ) + beta*C,alpha, beta

subroutine dgemm 	( 	character  	TRANSA,
		character  	TRANSB,
		integer  	M,
		integer  	N,
		integer  	K,
		double precision  	ALPHA,
		double precision, dimension(lda,*)  	A,
		integer  	LDA,
		double precision, dimension(ldb,*)  	B,
		integer  	LDB,
		double precision  	BETA,
		double precision, dimension(ldc,*)  	C,
		integer  	LDC 
	) 		

%https://software.intel.com/en-us/mkl-developer-reference-fortran-mkl-cscmm

C := alpha*A*B + beta*C
call mkl_dcscmm(transa, m, n, k, alpha, matdescra, val, indx, pntrb, pntre, b, ldb, beta, c, ldc)
\end{minted}

\section{Variable naming conventions}


This work uses the BLAS naming conventions wherever possible. These conventions must be extended to handle 
Firstly, iteration variable names are derived from the size variable names by adding the suffix \py{-i}. Secondly, we wish to iterate not only over cells, but also blocks and vectors. The naming convention handles this via a prefix. In effect, \py{m,n,k} determine the iteration \emph{direction}, and the prefix determines the units. Finally, we introduce the direction \py{l} to handle the third dimension, when working with tensors. The complete grammar is provided in Figure~\ref{fig:grammar}. This convention adds much-needed predictability and ease of comprehension inside deeply nested loops. It also helps ensure that iteration variables stay within the correct range. The most common pattern is to iterate over matrix blocks, and then inside each block iterate over cells (for sparse matrices) and vectors (for dense matrices). An example of this is given below:

\begin{minted}{python}
for ki in range(k):
	print(B[ki,ni])
\end{minted}

\begin{figure}[tbh]
  \centering
  \begin{subfigure}[l]{0.48\textwidth}
      \begin{minted}[linenos=false,fontsize=\small]{text}
    <var>       ::= [<units>]<direction>[i]
    <units>     ::= b | B | v | V
    <direction> ::= m | n | k | l
  \end{minted}
  \end{subfigure}
  ~~~~
  \begin{subfigure}[r]{0.45\textwidth}
    \centering
    \begin{tabular}{cll}
		\toprule
		Units    & Mnemonic & Meaning \\
		\midrule

		none  & Cell          & Cells in the matrix      \\
		b     & Intra-block   & Cells in a block         \\
		B     & Inter-block   & Blocks in the matrix     \\
		v     & Intra-vector  & Cells in a block         \\
		V     & Inter-vector  & Vectors in a block       \\
		\bottomrule
	\end{tabular}
  \end{subfigure}
  \caption{Grammar for variable names.}
  \label{fig:grammar}
\end{figure}


\section{The outer-product formulation}

Having subdivided a large matrix into blocks small enough to fit in registers, and having chosen the optimal traversal order for these blocks, it only remains to perform the multiplication of these blocks themselves. This is not trivial due to the constraints of working with vector registers. The state of the art is known as the outer-product formulation.

\begin{figure}[ht]
\begin{minted}[fontsize=\footnotesize]{gas}
# Load C register block
vmovapd 0(%rdx),  %zmm16                                 # load C[:,0]
vmovapd 64(%rdx), %zmm17                                 # load C[:,1]
vmovapd 128(%rdx), %zmm18                                # load C[:,2]
# ...

# Load A register block
vmovapd 0(%rdi), %zmm0                                   # load A[:,0]
vmovapd 64(%rdi), %zmm1                                  # load A[:,1]
# ...

# Outer product of A[:,0] * B[0,:]
vfmadd231pd 0(%rsi) {1to8}, %zmm0, %zmm16                # C[:,0] += A[:,0] .* B[0,0]
vfmadd231pd 0(%rsi,%r15,1) {1to8}, %zmm0, %zmm17         # C[:,1] += A[:,0] .* B[0,1]
vfmadd231pd 0(%rsi,%r15,2) {1to8}, %zmm0, %zmm18         # C[:,2] += A[:,0] .* B[0,2]
# ...

# Outer product of A[:,1] * B[1,:]
vfmadd231pd 8(%rsi) {1to8}, %zmm1, %zmm16                # C[:,0] += A[:,1] .* B[1,0]
vfmadd231pd 8(%rsi,%r15,1) {1to8}, %zmm1, %zmm17         # C[:,0] += A[:,1] .* B[1,1]
vfmadd231pd 8(%rsi,%r15,2) {1to8}, %zmm1, %zmm18         # C[:,0] += A[:,1] .* B[1,2]
# ...
# More outer products ...

# Store C register block
vmovapd %zmm16, 0(%rdx)                                  # store C[:,0]
vmovapd %zmm17, 64(%rdx)                                 # store C[:,1]
vmovapd %zmm18, 128(%rdx)                                # store C[:,2]
# ...
\end{minted}
\caption{Example outer-product formulation generated by libxsmm}
\label{fig:outerproduct}
\end{figure}

\section{Sparsity pattern unrolling}

\section{Memory addressing schemes}

The simplest scheme is to use pointer-offset addressing, where the pointer is in a register and the offset is hardcoded, such as \texttt{rsi + 22}. The downside of this approach is that when the offset is greater than 128*8, (i.e. there are more than 128 nonzeros in B), the resulting FMA instructions take up 10 bytes, which will cause a bottleneck as described in section X.XX. A more sophisticated scheme uses scale-index addressing, which uses two registers and two hardcoded values, such as \texttt{B + 8*i + 22}. These FMA instructions only take up 7 bytes; their main downside is that the generated code becomes harder to understand.




\section{Dense-by-sparse vs sparse-by-dense asymmetry}

- Outer-product formulation
- Breuer
- MKL
- Row-major vs Column-major


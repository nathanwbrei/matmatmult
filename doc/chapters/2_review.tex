\chapter{Background}
\label{chapter:review}

\section{Matrix multiplication}
Dense and spare matrix multiplication together account for two of the "seven pillars" of computation, in the sense that they encompass an unexpectedly large fraction of all computing, particularly in scientific and engineering fields. Since any algorithmic improvement has the potential to improve the performance of programs across the board, these algorithms have been highly optimized for each successive computer architecture and made available through a common interface known as BLAS since 1977. However, it wasn't until 2008 that a more general approach for designing optimized dense matrix multiplication algorithms -- independent of the exact details of the memory hierarchy -- was established. This work by Goto takes advantage of the recursive nature of block-wise matrix multiplication in order to successively divide the problem into smaller and smaller blocks, each of which is sized and padded to make best use of the memory hierarchy. Thus a very large matrix may be split among separate machines using Cannon's algorithm, then further split into blocks which fit in the L2 cache, then in the L1 cache, then finally in the registers themselves. 

\section{The outer-product formulation}

Having subdivided a large matrix into blocks small enough to fit in registers, and having chosen the optimal traversal order for these blocks, it only remains to perform the multiplication of these blocks themselves. This is not trivial due to the constraints of working with vector registers. The state of the art is known as the outer-product formulation.

\begin{figure}[ht]
\begin{minted}[fontsize=\footnotesize]{gas}
# Load C register block
vmovapd 0(%rdx),  %zmm16                                 # load C[:,0]
vmovapd 64(%rdx), %zmm17                                 # load C[:,1]
vmovapd 128(%rdx), %zmm18                                # load C[:,2]
# ...

# Load A register block
vmovapd 0(%rdi), %zmm0                                   # load A[:,0]
vmovapd 64(%rdi), %zmm1                                  # load A[:,1]
# ...

# Outer product of A[:,0] * B[0,:]
vfmadd231pd 0(%rsi) {1to8}, %zmm0, %zmm16                # C[:,0] += A[:,0] .* B[0,0]
vfmadd231pd 0(%rsi,%r15,1) {1to8}, %zmm0, %zmm17         # C[:,1] += A[:,0] .* B[0,1]
vfmadd231pd 0(%rsi,%r15,2) {1to8}, %zmm0, %zmm18         # C[:,2] += A[:,0] .* B[0,2]
# ...

# Outer product of A[:,1] * B[1,:]
vfmadd231pd 8(%rsi) {1to8}, %zmm1, %zmm16                # C[:,0] += A[:,1] .* B[1,0]
vfmadd231pd 8(%rsi,%r15,1) {1to8}, %zmm1, %zmm17         # C[:,0] += A[:,1] .* B[1,1]
vfmadd231pd 8(%rsi,%r15,2) {1to8}, %zmm1, %zmm18         # C[:,0] += A[:,1] .* B[1,2]
# ...
# More outer products ...

# Store C register block
vmovapd %zmm16, 0(%rdx)                                  # store C[:,0]
vmovapd %zmm17, 64(%rdx)                                 # store C[:,1]
vmovapd %zmm18, 128(%rdx)                                # store C[:,2]
# ...
\end{minted}
\caption{Example outer-product formulation generated by libxsmm}
\label{fig:outerproduct}
\end{figure}

\section{Small dense matrix kernels}

\section{Sparsity pattern unrolling}

\section{Memory addressing schemes}

The simplest scheme is to use pointer-offset addressing, where the pointer is in a register and the offset is hardcoded, such as \texttt{rsi + 22}. The downside of this approach is that when the offset is greater than 128*8, (i.e. there are more than 128 nonzeros in B), the resulting FMA instructions take up 10 bytes, which will cause a bottleneck as described in section X.XX. A more sophisticated scheme uses scale-index addressing, which uses two registers and two hardcoded values, such as \texttt{B + 8*i + 22}. These FMA instructions only take up 7 bytes; their main downside is that the generated code becomes harder to understand.




\section{Dense-by-sparse vs sparse-by-dense asymmetry}

\section{Variable naming conventions}


There exist well-established conventions for naming variables inside matrix multiplication algorithms. 
The use of the letters m,n,k is unavoidable. However, the conventions become less clear when the loops are nested over blocks, vectors, and cells.
Another point of confusion is naming iteration variables. This work adopts the following naming conventions:

  \begin{minted}[fontsize=\footnotesize]{text} 
    <var>       ::= [<units>] <dimension> [i]
    <units>     ::= b | B | v | V
    <dimension> ::= m | n | k | l
  \end{minted}

The units indicates what is being counted or iterated over:

    \begin{tabular}{cll}
\toprule
Units    & Mnemonic & Meaning \\
\midrule

none  & Cell          & Cells in the matrix      \\
b     & Intra-block   & Cells in a block         \\
B     & Inter-block   & Blocks in the matrix     \\
v     & Intra-vector  & Cells in a block         \\
V     & Inter-vector  & Vectors in a block       \\
\bottomrule
\end{tabular}

The dimension indicates 

\begin{tabular}{cl}
\toprule
Dimension    & Meaning \\
\midrule
m & Rows of A and C \\
n & Columns of B and C \\
k & Columns of A, rows of B \\
l & The third dimension of A and C\\
\bottomrule
\end{tabular}


Finally, 'i' indicates that this is an iteration variable.
For example, one may confidently assume that the variable $bni \in [0, bn-1]$ iterates cell-by-cell across columns of B and C within the current block.


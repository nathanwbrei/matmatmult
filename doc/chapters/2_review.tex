\chapter{Review of Concepts}
\label{chapter:review}

\section{Matrix multiplication}
Dense and spare matrix multiplication together account for two of the "seven pillars" of computation, in the sense that they encompass an unexpectedly large fraction of all computing, particularly in scientific and engineering fields. Since any algorithmic improvement has the potential to improve the performance of programs across the board, these algorithms have been highly optimized for each successive computer architecture and made available through a common interface known as BLAS since 1977. However, it wasn't until 2008 that a more general approach for designing optimized dense matrix multiplication algorithms -- independent of the exact details of the memory hierarchy -- was established. This work by Goto takes advantage of the recursive nature of block-wise matrix multiplication in order to successively divide the problem into smaller and smaller blocks, each of which is sized and padded to make best use of the memory hierarchy. Thus a very large matrix may be split among separate machines using Cannon's algorithm, then further split into blocks which fit in the L2 cache, then in the L1 cache, then finally in the registers themselves. 

\section{The outer-product formulation}

Having subdivided a large matrix into blocks small enough to fit in registers, and having chosen the optimal traversal order for these blocks, it only remains to perform the multiplication of these blocks themselves. This is not trivial due to the constraints of working with vector registers. The state of the art is known as the outer-product formulation.

\begin{figure}[ht]
\begin{minted}[fontsize=\footnotesize]{gas}
# Load C register block
vmovapd 0(%rdx),  %zmm16                                 # load C[:,0]
vmovapd 64(%rdx), %zmm17                                 # load C[:,1]
vmovapd 128(%rdx), %zmm18                                # load C[:,2]
# ...

# Load A register block
vmovapd 0(%rdi), %zmm0                                   # load A[:,0]
vmovapd 64(%rdi), %zmm1                                  # load A[:,1]
# ...

# Outer product of A[:,0] * B[0,:]
vfmadd231pd 0(%rsi) {1to8}, %zmm0, %zmm16                # C[:,0] += A[:,0] .* B[0,0]
vfmadd231pd 0(%rsi,%r15,1) {1to8}, %zmm0, %zmm17         # C[:,1] += A[:,0] .* B[0,1]
vfmadd231pd 0(%rsi,%r15,2) {1to8}, %zmm0, %zmm18         # C[:,2] += A[:,0] .* B[0,2]
# ...

# Outer product of A[:,1] * B[1,:]
vfmadd231pd 8(%rsi) {1to8}, %zmm1, %zmm16                # C[:,0] += A[:,1] .* B[1,0]
vfmadd231pd 8(%rsi,%r15,1) {1to8}, %zmm1, %zmm17         # C[:,0] += A[:,1] .* B[1,1]
vfmadd231pd 8(%rsi,%r15,2) {1to8}, %zmm1, %zmm18         # C[:,0] += A[:,1] .* B[1,2]
# ...
# More outer products ...

# Store C register block
vmovapd %zmm16, 0(%rdx)                                  # store C[:,0]
vmovapd %zmm17, 64(%rdx)                                 # store C[:,1]
vmovapd %zmm18, 128(%rdx)                                # store C[:,2]
# ...
\end{minted}
\caption{Example outer-product formulation generated by libxsmm}
\label{fig:outerproduct}
\end{figure}

\section{Small dense matrix kernels}



\section{Sparsity pattern unrolling}



\section{Dense-by-sparse vs sparse-by-dense asymmetry}


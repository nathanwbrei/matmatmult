\chapter{Background}
\label{chapter:review}

\section{Matrix multiplication}

Dense and sparse matrix multiplication account for a surprising fraction of all computation, particularly in scientific and engineering fields. Since any optimization or algorithmic improvement has the potential to improve the performance of programs across the board, these algorithms have been organized under a series of common interfaces. Not only does this make it easy to reuse the best known implementation of many fundamental linear algebra operations, but it also confers other advantages: robustness is increased by the careful handling of edge cases, portability is increased because a programmer can link to the best implementation for their target architecture, and readability is increased because numerical algorithms can all work at the same abstraction level. The first such interface, \gls{BLAS}, was introduced in 1979 by Lawson~\cite{Lawson:1979:BLA:355841.355847} and only supported dense vector and matrix operations. Later interfaces, such as LAPACK, build upon and extend \gls{BLAS} to support increasingly complex operations; the history and design of such interfaces is covered by Dongarra in~\cite{Dongarra:1998:NLA:552704}. 

Most of the naming conventions used in this work follow those of BLAS and its successors. The arguments for each operation get quite complicated (see~\cite{IntelCSCMM}), but for our purposes they can be simplified as shown in Figure~\ref{fig:blas}. These conventions have propagated to libraries including MKL and libxsmm.

\begin{figure}
\begin{verbatim}
// C = alpha*A*B + beta*C
dgemm(m, n, k, alpha, A, lda, B, ldb, beta, C, ldc)
\end{verbatim}

\begin{tabular}{ll}
	\toprule
	Variable    &  Meaning \\
	\midrule
	dgemm & Double General Matrix Multiplication \\
	alpha & A scalar constant \\
	beta  & A scalar constant \\
	A     & A matrix of doubles, in dense column-major format unless sparse \\
	B     & A matrix of doubles, in dense column-major format unless sparse \\
	C     & A matrix of doubles, in dense column-major format \\
	m     & The number of rows of C and A \\
	n     & The number of columns of C and B \\
	k     & The number of columns in A and rows in B \\
	lda   & The leading dimension (offset between columns) of A; zero indicates sparse \\
	ldb   & The leading dimension (offset between columns) of B; zero indicates sparse \\
	ldc   & The leading dimension (offset between columns) of C \\


	\bottomrule
\end{tabular}
\caption{Simplified BLAS naming conventions for general matrix multiplication}
\label{fig:blas}
\end{figure}


A sparse BLAS interface was not standardized until 2002, as described by Duff in~\cite{Duff:2002:OSB:567806.567810}. Compared with the dense case, sparse matrix operations are considerably more difficult to pin down because the performance of each operation depends heavily -- often asymptotically -- on the memory layout, which in turn depends on both the chosen storage format and the sparsity pattern, which is often known only at runtime. The choice of storage format is tightly coupled with the implementation of the operations, making abstraction difficult. Thus there are a multitude of different storage formats, with not entirely consistent naming conventions. The formats used in this work are described in Figure~\ref{fig:formats} and illustrated throughout Chapter~\ref{chapter:algs}. Ultimately, a sparse BLAS works well when the matrices are large, are constrained to a common format such as CSC, and have unpredictable or nonconstant sparsity patterns; otherwise a custom implementation might be able to do much better.

\begin{figure}

\begin{description}
	\item[Dense (DNS)] Column-major dense format, possibly padded
	\item[Coordinate format (COO)] 
	\item[Compressed sparse column (CSC)] Nonzero entries are stored column-by-column left to right. 
	\item[Compressed sparse row (CSR)]
	\item[Block sparse column (BSC)]
	\item[Block sparse row (BSR)]
	\item[Block compressed sparse column (BCSC)]
	\item[Block compressed sparse row (BCSR)]
	\item[Diagonal (DIA)]
	\item[Virtual... (V...)] Any sparse format where the index information has been omitted.
\end{description}


\caption{Descriptions of different sparse matrix formats as used in this work. Naming conventions vary between literature; pay attention to the distinction between BSC and BCSC. }
\label{fig:formats}
\end{figure}

What did Goto figure out?

However, it wasn't until 2008 that a more general approach for designing optimized dense matrix multiplication algorithms -- independent of the exact details of the memory hierarchy -- was established. This work by Goto takes advantage of the recursive nature of block-wise matrix multiplication in order to successively divide the problem into blocks, each of which is sized and padded to make best use of the memory hierarchy. Thus a very large matrix may be split among separate machines using Cannon's algorithm, then further split into blocks which fit in the L2 cache, then in the L1 cache, then finally in the registers themselves. 

Goto's work , GotoBLAS, influenced BLIS. 


Why doesn't Goto's work apply here?

What is the outer-product formulation?

What does Libxsmm do differently from BLAS/Goto?

What about memory-addressing schemes

What about Breuer's work? Does it vectorize?

What is the sparse/dense asymmetry?

What naming conventions do we use?



\section{Variable naming conventions}


This work uses the BLAS naming conventions wherever possible. These conventions must be extended to handle 
Firstly, iteration variable names are derived from the size variable names by adding the suffix \py{-i}. Secondly, we wish to iterate not only over cells, but also blocks and vectors. The naming convention handles this via a prefix. In effect, \py{m,n,k} determine the iteration \emph{direction}, and the prefix determines the units. Finally, we introduce the direction \py{l} to handle the third dimension, when working with tensors. The complete grammar is provided in Figure~\ref{fig:grammar}. This convention adds much-needed predictability and ease of comprehension inside deeply nested loops. It also helps ensure that iteration variables stay within the correct range. The most common pattern is to iterate over matrix blocks, and then inside each block iterate over cells (for sparse matrices) and vectors (for dense matrices). An example of this is given below:

\begin{minted}{python}
for ki in range(k):
	print(B[ki,ni])
\end{minted}

\begin{figure}[tbh]
  \centering
  \begin{subfigure}[l]{0.48\textwidth}
      \begin{minted}[linenos=false,fontsize=\small]{text}
    <var>       ::= [<units>]<direction>[i]
    <units>     ::= b | B | v | V
    <direction> ::= m | n | k | l
  \end{minted}
  \end{subfigure}
  ~~~~
  \begin{subfigure}[r]{0.45\textwidth}
    \centering
    \begin{tabular}{cll}
		\toprule
		Units    & Mnemonic & Meaning \\
		\midrule

		none  & Cell          & Cells in the matrix      \\
		b     & Intra-block   & Cells in a block         \\
		B     & Inter-block   & Blocks in the matrix     \\
		v     & Intra-vector  & Cells in a block         \\
		V     & Inter-vector  & Vectors in a block       \\
		\bottomrule
	\end{tabular}
  \end{subfigure}
  \caption{Grammar for variable names.}
  \label{fig:grammar}
\end{figure}


\section{The outer-product formulation}

Having subdivided a large matrix into blocks small enough to fit in registers, and having chosen the optimal traversal order for these blocks, it only remains to perform the multiplication of these blocks themselves. This is not trivial due to the constraints of working with vector registers. The state of the art is known as the outer-product formulation.

\begin{figure}[ht]
\begin{minted}[fontsize=\footnotesize]{gas}
# Load C register block
vmovapd 0(%rdx),  %zmm16                                 # load C[:,0]
vmovapd 64(%rdx), %zmm17                                 # load C[:,1]
vmovapd 128(%rdx), %zmm18                                # load C[:,2]
# ...

# Load A register block
vmovapd 0(%rdi), %zmm0                                   # load A[:,0]
vmovapd 64(%rdi), %zmm1                                  # load A[:,1]
# ...

# Outer product of A[:,0] * B[0,:]
vfmadd231pd 0(%rsi) {1to8}, %zmm0, %zmm16                # C[:,0] += A[:,0] .* B[0,0]
vfmadd231pd 0(%rsi,%r15,1) {1to8}, %zmm0, %zmm17         # C[:,1] += A[:,0] .* B[0,1]
vfmadd231pd 0(%rsi,%r15,2) {1to8}, %zmm0, %zmm18         # C[:,2] += A[:,0] .* B[0,2]
# ...

# Outer product of A[:,1] * B[1,:]
vfmadd231pd 8(%rsi) {1to8}, %zmm1, %zmm16                # C[:,0] += A[:,1] .* B[1,0]
vfmadd231pd 8(%rsi,%r15,1) {1to8}, %zmm1, %zmm17         # C[:,0] += A[:,1] .* B[1,1]
vfmadd231pd 8(%rsi,%r15,2) {1to8}, %zmm1, %zmm18         # C[:,0] += A[:,1] .* B[1,2]
# ...
# More outer products ...

# Store C register block
vmovapd %zmm16, 0(%rdx)                                  # store C[:,0]
vmovapd %zmm17, 64(%rdx)                                 # store C[:,1]
vmovapd %zmm18, 128(%rdx)                                # store C[:,2]
# ...
\end{minted}
\caption{Example outer-product formulation generated by libxsmm}
\label{fig:outerproduct}
\end{figure}

\section{Sparsity pattern unrolling}

\section{Memory addressing schemes}

The simplest scheme is to use pointer-offset addressing, where the pointer is in a register and the offset is hardcoded, such as \texttt{rsi + 22}. The downside of this approach is that when the offset is greater than 128*8, (i.e. there are more than 128 nonzeros in B), the resulting FMA instructions take up 10 bytes, which will cause a bottleneck as described in Section~\ref{section:knl}. A more sophisticated scheme uses scale-index addressing, which uses two registers and two hardcoded values, such as \texttt{B + 8*i + 22}. These FMA instructions only take up 7 bytes; their main downside is that the generated code becomes harder to understand.




\section{Dense-by-sparse vs sparse-by-dense asymmetry}
(Dense $\times$ Sparse) $\neq$ (Sparse $\times$ Dense) due to vectorization

Choice of row-major vs column-major format determines which of the two vectorizes nicely: $(AB)^T = B^T A^T$

- Outer-product formulation
- Breuer
- MKL
- Row-major vs Column-major


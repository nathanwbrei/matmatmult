\chapter{Algorithms}
\label{chapter:algs}

The main focus of this thesis is on improving the performance of small matrix multiplication algorithms. It is necessary to finesse around the bottlenecks that constrain the approaches described in the previous chapter. The extra information which makes this possible is the sparsity pattern known at compile time. The focus is on dense-by-sparse multiplication because it stands to benefit the most from vectorization, although sparse-by-dense and tensor-by-sparse multiplication are also discussed. Figure~\ref{fig:dxspfamilies} shows the different dense-by-sparse algorithms under consideration and the relationships between them. 

The starting point is the dense outer-product formulation generated from \texttt{libxsmm}. Applying knowledge about the sparsity pattern yields MicroSparse, which avoids wasted computation but is restricted to very small matrix sizes. Decomposing a matrix into blocks, using MicroSparse for each block, and unrolling some of the outer loops yields UnrolledSparse, which is not restricted by matrix dimensions but is restricted by the number of nonzeros. In order to support matrices with many nonzeros, some regularity assumptions have to be made. 

\begin{figure}[tbh]
  \centering
  \input{images/dxspfamilies}
  \caption{Relationships between different dense-by-sparse multiplication algorithms}
  \label{fig:dxspfamilies}
\end{figure}


The simplest approach is TiledSparse, which assumes that a single block pattern is repeated exactly. This assumption is impractical because for most matrix patterns, the block pattern will fill in. Instead we can break the assumption into two parts and relax them separately. We can assume that there is only one block sparsity pattern, but some blocks are entirely zero, which yields the BlockedSparse algorithm. Alternatively, we can assume that there are multiple block sparsity patterns, but they all have the same number of nonzeros, which yields PatternSparse. Relaxing both assumptions yields GeneralSparse, which can evade the number of nonzeros constraint with the most flexibility, but at the cost of additional overhead. Variations can be explored which extract the diagonal and handle it separately; this would be especially helpful for PaddedSparse in the case of matrices with a full diagonal and random fill-in elsewhere.

\section{Dense-by-sparse microkernel}

The MicroSparse kernel is our first attempt to account for the sparsity pattern while building off of the state of the art for dense matrix multiplication. Our key assumption is that the matrices A and C are small enough to fit entirely in registers: $(n + k) * (m/8) <= 32$. Generating the dense GEMM for this problem effectively extracts the dense microkernel. This microkernel has no loops (otherwise it couldn't amortize the cost of loading and storing A and C) and this makes it easy to modify.

The sparse microkernel is a straightforward modification of the dense microkernel. All FMA instructions \py{C[:,ni] += A[:,ki] .* B[ki, ni]} where \py{B[ki, ni] = 0} due to the sparsity pattern are removed. This is tricky to do in a general way; an abstraction designed for the purpose is described at length in Section~\ref{section:cursors}. Next, any loads of a column of A corresponding to an empty row of B are removed. Finally, any loads and stores of a column of C corresponding to an empty column of B are removed. This is described via pseudocode in Figure~\ref{fig:micropseudo}.

\begin{figure}[htb]
\centering
  \begin{minted}[linenos=false,fontsize=\footnotesize]{text}

    for each vector Vmi:
        for each cell ni:
            if B[:,ni] != 0:
                emit load C[Vmi, ni]

    for each vector Vmi:
        for each cell ki:
            if B[ki,:] != 0:
                emit load A[Vmi, ki]

    for each vector Vmi:
        for each cell ki:
            for each cell ni:
                if B[ki,ni] != 0:
                    emit fma C[Vmi, ni] += A[Vmi, ki] .* B[ki, ni]

    for each vector Vmi:
        for each cell ni:
            if B[:,ni] != 0:
                emit store C[Vmi, ni]

  \end{minted}
  \caption{Pseudocode for MicroSparse}
\label{fig:micropseudo}
\end{figure}

Before proceeding to more complex algorithms, it is worthwhile to consider the degrees of freedom available. The first parameter to consider is the format of the sparse matrix B. Because the memory offsets of each \py{B[ki,ni]} are hardcoded into the FMA, any format will work as long as the pointer to B never moves. However, the choice of format strongly affects memory access patterns. Keeping B dense is suboptimal because the algorithm will be streaming nonzeros which are unduly spread across different cache lines. Breuer and Heinecke used a virtual compressed sparse column format, where the nonzeros were laid out in a plain 1D array ordered by columns. For MicroSparse, a virtual compressed row format might have better locality because the $ni$ loop is innermost, but this is insignificant because few cache lines are involved on sparse matrices of this size. 

Having chosen the format, the next step is to choose the addressing mode for the elements of the sparse matrix. For now we assume that B points to the first entry in this matrix and the entries are all adjacent. Because the maximum number of doubles in B is 256 and the cutoff for 7-byte FMAs is 128, it already makes sense to switch to scale-index addressing. Because this is complicated to do, it will be hidden behind the cursor abstraction described in Section~\ref{section:cursors}.

Another question is alignment. The alignment of B is not particularly important because its elements are loaded one at a time, but very important for A and C, which are loaded in a vectorwise fashion. If not aligned on a 64-byte boundary, the vector load operation will suffer a delay because the data is split across two cache lines. Using the aligned vector load operation \py{vmovapd} will trigger a segmentation fault if the pointer is not actually aligned. On the other hand, the unaligned version \py{vmovupd} can handle both the aligned and unaligned case, and will not incur the penalty if the pointer is in fact aligned. Fortunately, this choice can be made independently of any other considerations.

The performance of MicroSparse is demonstrated in Section~\ref{section:exp_unrolled_scaling}. It is clear that removing the FMA instructions decreases the arithmetic intensity, and if the nonzeros are evenly distributed, there will be no accompanying decrease in memory bandwidth. Nevertheless the algorithm remains compute-bound and experiences an improved time-to-solution compared to its dense counterpart. Regardless of performance, MicroSparse has limited utility in real problems, since it can only be applied to extremely small matrices. It is valuable primarily as a building block for the more complex algorithms developed next.


\section{Unrolled dense-by-sparse multiplication}


The goal of UnrolledSparse is to support larger matrix sizes by building off of MicroSparse. The first key idea is to introduce a block decomposition: the generators traverse blocks $(Bmi, Bni, Bki)$ and delegate the multiplication of each block to MicroSparse. The second key idea is to account for the fact that different blocks contain different subpatterns by unrolling the $Bni$ and $Bki$ loops. The sparsity pattern is obviously invariant with respect to $Bmi$, so it is not necessary to unroll that loop. 

It is essential that $Bki$ be the innermost loop because this lets us accumulate changes to the block \py{C[Bmi, Bni]}, and then store the entire block exactly once. Otherwise, the repeated storing of each block of C becomes the primary bottleneck. This also means that we only reuse the inner part of MicroSparse, and we handle the loads and stores of C ourselves.

Putting the $Bmi$ loop on the outside is the best choice for several reasons. Firstly, it consolidates instructions that are free of control dependencies, which improves pipeline efficiency and gives the processor more room to amortize loads of A and C. Secondly, it dramatically reduces the number of branches. Even if branches are predicted perfectly, KNL's issue-width bottleneck ensures that the instructions to increment and compare the counter register would displace FMAs, reducing performance.

Thus there is only one reasonable ordering for our loop nest. At the innermost level we unroll over the $Bki$ loop, laying down a sequence of sparse microkernels corresponding to a traversal of one panel of B, moving from top to bottom in short, fat slices. In the middle level, we unroll over $Bni$, corresponding to a traversal across panels of B from left to right. Finally, at the outermost level, we loop over $Bmi$: horizontal panels of A and C from top to bottom. This is described more precisely via pseudocode in Figure~\ref{fig:pseudocode_unrolled}.

This loop nesting looks very similar to what Kazushige Goto~\cite{Goto:2008:AHM:1356052.1356053} calls GEPM\_VAR2. Note that Goto rejects this algorithm within one paragraph, making the exact opposite argument as we have made above, regarding the repeated storing of the C block. This is because Goto is focusing on a much larger scale: his C blocks fit only in the L2 cache, whereas ours fit entirely in registers: we get our inner ``GEPDot'' for free whereas his is especially costly.

The UnrolledSparse algorithm has three additional tuning parameters which must be chosen. These are the blocksizes $bm$, $bn$, and $bk$. Section~\ref{section:exp_unrolled_sizing} explores how to choose these values heuristically. The choice of parameters and the set of problems which can be efficiently solved are subject to the following constraints:

\begin{itemize}
  \item The A and C blocks must fit in registers: $(bk + bn) * (bm / 8) <= 32$
  
  \item Each nonzero in B produces exactly one FMA, and if there are too many of these, they will fill the instruction cache. This introduces a hard limit on the number of nonzeros, which shall be explored in depth in Section~\ref{section:exp_unrolled_scaling}
  
  \item The instruction cache bottleneck can be hit with fewer nonzeros depending on the choice of blocksizes, since different blocksizes result in different numbers of \py{vmovapd} instructions. This is covered in Section~\ref{section:exp_unrolled_sizing}.

  \item Choices of $(m,n,k)$ which do not evenly divide into small blocks are feasible, but not fully supported at the time of this writing.
\end{itemize}

The experiments discussed later consistently show that UnrolledSparse strikes a good balance between performance and flexibility. It supports most available matrix sizes, it does not fill in any zeros or waste any flops, and it uses a dense-style register blocking. It continues to perform well as the B matrix becomes dense -- the parallel efficiency increases -- and it degenerates to something similar to libxsmm. For fully dense matrices it performs slightly worse than libxsmm, and if it reaches the instruction cache limit it performs dramatically worse. In order to move past this constraint, we find ways to compress the number of FMAs below the number of actual nonzeros.

\begin{figure}[tb]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
          \[
      \left[
          \begin{array}{c c c | c c c }
          0 &   &   & 10 &   &   \\
          1 & 2 &   & 11& 12&   \\
            & 3 & 4 &   & 13& 14  \\
          \hline
          5 &   &   & 15  &   &   \\
          6 & 7 &   & 16  &17   &   \\
            & 8 & 9 &   & 18  &19   \\
          \end{array}
          \right]
      \]
    \caption{TiledSparse}
    \label{fig:tiledblocks}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
      \[
      \left[
          \begin{array}{c c c | c c c }
          0 &   &   &    &    &    \\
          1 & 2 &   &    &    &    \\
            & 3 & 4 &    &    &    \\
          \hline
          5 &   &   & 10 &    &    \\
          6 & 7 &   & 11 & 12 &    \\
            & 8 & 9 &    & 13 & 14 \\
          \end{array}
          \right]
      \]    \caption{BlockedSparse}
    \label{fig:blockedblocks}
  \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
    \centering
      \[
      \left[
          \begin{array}{c c c | c c c }
          0 &   &   &    &    &    \\
          1 & 2 &   &    &    &    \\
            & 3 & 4 &    &    &    \\
          \hline
          5  & 6  & 7  &    &  14& 15  \\
          8  & 9  & 10 &    &    & 16 \\
          11 & 12 & 13 &    &    &    \\
          \end{array}
          \right]
      \]    \caption{GeneralSparse}
    \label{fig:generalblocks}
  \end{subfigure}
  \caption{Sample matrices illustrating the blocking constraints for different GEMM algorithms. TiledSparse admits only a single block pattern which is tiled perfectly over the entire matrix. BlockedSparse admits a single block pattern along with empty blocks. GeneralSparse relaxes further to admit multiple different patterns. The numbers indicate the cell's index when using a block compressed-sparse-row format. }
  \label{fig:matrixblocks}

\end{figure}




\begin{figure}[ht]
  \begin{subfigure}[b]{0.45\textwidth}
      \begin{minted}[linenos=false,fontsize=\footnotesize]{text}
        loop over m blocks:

           unroll over n blocks:

              load C block into registers

              unroll over k blocks:

                 load A block into registers
                 blockwise C += A * B
                 move A right 1 block
                 move B down 1 block

              store C block from registers
              move A to far left
              move C right 1 block

           move A down 1 block
           move C far left, down 1 block
      \end{minted}    
      \caption{Pseudocode for UnrolledSparse}
    \label{fig:pseudocode_unrolled}
  \end{subfigure}
  ~~~~~~
  \begin{subfigure}[b]{0.45\textwidth}
      \begin{minted}[linenos=false,fontsize=\footnotesize]{text}
        loop over m blocks:

           loop over n blocks:

              load C block into registers

              unroll over k blocks:

                 load A block into registers
                 blockwise C += A * B
                 move A right 1 block
                 move B down 1 block

              store C block from registers
              move A to far left
              move B to top + right 1 block
              move C right 1 block

           move A down 1 block
           move C far left, down 1 block
      \end{minted}    
      \caption{Pseudocode for TiledSparse}
    \label{fig:pseudocode_tiled}

  \end{subfigure}
    \caption{Pseudocode for UnrolledSparse and TiledSparse. UnrolledSparse unrolls two inner loops, emitting a microkernel for every block of B. TiledSparse unrolls only the innermost loop, emitting a microkernel for every (identical) block in one panel of B. Note that the `unroll' command replaces `move' statements with hardcoded memory addresses when possible.}
    \label{fig:pseudocode_unrolled_tiled}

\end{figure}



\section{Tiled dense-by-sparse multiplication}

The basic idea behind TiledSparse is to scale the FullyUnrolledSparse algorithm to matrices with more nonzeros than the instruction cache permits by assuming that the sparsity pattern is perfectly regular. Specifically, we assume that there is a single sparsity pattern which tiles over the entire matrix and is small enough to be used by MicroSparse. Our tiling assumption can be made without loss of generality because we can always fill in zeros until we arrive at a regular pattern. One way to do this is to partition the original matrix with the desired blocksize, and then take the logical union of the sparsity patterns in each block. The assumption does result in a loss of practicality, however, as often the pattern will degenerate to dense. If the pattern becomes dense, the resulting algorithm will be more or less similar to libxsmm depending on the choice of block sizes.

We now decide the order in which the loops are nested, and also which loops are unrolled. As with UnrolledSparse, the $Bki$ loop should be the innermost in order to accumulate updates to C, and it is advantageous to unroll it unless the matrix is very large and not very sparse, in which case it may still be advantageous to partially unroll it (though this is not yet supported). The $Bni$ loop can not be unrolled, however, because this would once again yield one FMA per nonzero and we would be back to UnrolledSparse. Unrolling the $Bmi$ loop is similarly likely to curtail this algorithm's scaling to higher nnzs, albeit with less predictability. There is little expected benefit to interchanging the $Bmi$ and $Bni$ loops, but we are free to do so.

Pseudocode showing TiledSparse is provided in Figure~\ref{fig:pseudocode_tiled}. Intriguingly, the only difference between UnrolledSparse and TiledSparse (at the abstraction level shown) is the $Bni$ loop is no longer unrolled. However, this conceals another important difference. When the $Bni$ loop is unrolled, the commands to move the B pointer gets completely absorbed into the memory addresses, whose offsets are absolute. When the $Bni$ loop is not unrolled, we must move the B pointer to the start of the corresponding panel, in which case the offsets are relative. This is a simple add operation, but it requires that each block takes up the same, expected amount of space in memory. Depending on the origin of the data, it might be a challenge to enforce that the layout in memory is consistent with the tiled sparsity pattern.

Pattern-specific zero-padding of sparse matrices is not something numerical libraries typically allow, but a specialized class can be provided for that purpose. If this class stores the matrix in COO format, and does so using a structure of arrays, it can mimic any virtual storage format by simply reordering the entries. Given an unpadded matrix of numeric data alongside a matrix containing the sparsity pattern, it can construct a new matrix which contains both the numeric data and the correct zero padding in an arbitrary format. 

There are several improvements possible. TiledSparse can be modified to support a non-perfect tiling with a fringe on the bottom and right. It is even possible to remove the block size constraint by replacing the MicroSparse kernel with an UnrolledSparse kernel. Nevertheless, it will take much bigger modifications to escape the problem of the sparsity pattern filling to dense.




\section{General dense-by-sparse multiplication}

The general dense-by-sparse algorithm comes from relaxing \emph{both} of the key ideas of TiledSparse: There can be any number of different block patterns, and each pattern may contain any number of nonzeros. GeneralSparse contains a collection of MicroSparse GEMMs, and chooses the correct GEMM for each block of B. It is free to scale to larger matrix sizes than those supported by UnrolledSparse, because it can be compressed. Judiciously filling in zeros can arbitrarily reduce the number of unique block patterns, which in turn reduces the number of GEMMs. Thus GeneralSparse degrades gracefully, similar to TiledSparse. As the matrices grow larger and denser, the algorithm will converge towards either BlockedSparse or dense, depending on the original sparsity pattern. In contrast to TiledSparse and BlockedSparse, however, the fill-in (and hence wasted FLOPs) can be effectively minimized.

There is of course a tradeoff: in order to redirect control flow to the correct block GEMM, one direct and one indirect jump are needed. This must happen for every non-empty block of B. Each indirect jump will add latency; this is analyzed and estimated in Section~\ref{section:exp_jump_scaling}. It might be possible to reduce this latency by putting padding around the jump instructions; this has not yet been tried.

To efficiently handle the multiway control flow, we start with a low-level pattern called a \emph{jump table}. Assume the control variable \py{x} is a small integer. The destination addresses are laid out in an array \py{A}, and control is transferred by jumping to the address stored in \py{A[x]}. This happens in constant time regardless of the size of \py{x}.

There are two different ways we can apply this to our problem. One approach involves putting all of the MicroSparse kernels inside of a loop nest over $Bn$ and $Bk$. Somewhere else, we lay out a $Bk \times Bn$ array which contains the address of the kernel corresponding to each block. The loop nest updates pointers to the start of the active blocks of A and C, updates the pointer into the address matrix, and then performs a single indirect jump using that pointer. Note that either the MicroSparse kernel must be responsible for moving B to the start of the next block, or the starting locations of each block must be stored in another array.

The second approach involves unrolling the $Bn$ and $Bk$ loops completely. For each block, we move the A, B, C pointers to the start of the active block, and save a return address to a register. This is effectively a (non-ABI-compliant) function call, except it avoids the stack completely. We perform a direct jump to the correct block kernel. The kernel itself performs the indirect jump to the return address stored in the register. This approach is more performant because it avoids doing the jump when it knows that the current block is empty.


GeneralSparse is subject to following constraints:

\begin{enumerate}

  \item The total number of indirect jumps should be minimized. This favors a larger block size.

  \item The routine must fit in the 32kB instruction cache, which constrains the sum of nonzeros in each \emph{unique} block. 

  \item The sparse matrix must be stored in \emph{block} compressed sparse row or column format. Otherwise, the offsets of the entries in a block (relative to the start of that block) are no longer constant, and the microkernel returns incorrect results. 

  \item Blocks should be uniformly sized. This is an implementation detail; the constraint may be removed in the future by implementing another MatrixCursor as described in Section~\ref{section:cursors}.

\end{enumerate}

Automatically choosing parameters for GeneralSparse is tricker than for the preceding kernels. Ironically, the only parameters needed are a valid choice of $bn$ and $bk$. To minimize the jump penalty, they should be chosen as large as possible. However, for any particular choice, the algorithm cannot estimate its performance until it has compressed the matrix-wide sparsity pattern. This can become computationally expensive.

The aforementioned sparsity pattern compression can be accomplished with a greedy algorithm. It maintains a set of unique block patterns that together cover all of the blocks. As long as the sum of nonzeros in this set exceeds the instruction cache limit, the algorithm chooses the two which are most similar according to logical XOR, and then merges them using logical OR.



\section{Algorithms not implemented}

\subsection{Blocked dense-by-sparse multiplication}

  The key idea behind BlockedSparse was to partially relax the TiledSparse assumption. Specifically, we would allow only one block sparsity pattern, but also allow empty blocks. This is reminiscent of the existing BSR format, except the blocks are internally sparse instead of dense. 

  This is ultimately just a simplified case of GeneralSparse. The additional assumptions mean that the generated code is much shorter because it contains only one MicroSparse kernel, and that the indirect jump can be replaced with a direct jump. Neither of these changes is expected to yield a substantial performance increase. The overwhelming majority of matrices will perform worse on BlockedSparse compared to GeneralSparse due to additional fill-in.

  We can assume that there is only one block sparsity pattern, but some blocks are entirely zero, which yields the BlockedSparse algorithm. Alternatively, we can assume that there are multiple block sparsity patterns, but they all have the same number of nonzeros, which yields PatternSparse.


\subsection{Padded dense-by-sparse multiplication}

  PaddedSparse partially relaxes the other TiledSparse assumption -- there may be multiple unique block patterns, but there is always a constant offset between blocks in memory. This is achieved by padding zeros in memory similar to how TiledSparse fills in its block pattern. Unlike TiledSparse, this doesn't affect the flop count; it merely spreads the nonzeros out in memory. The principal advantage to this approach is that it compresses sparsity patterns like GeneralSparse, but is safe to use with the CSC and CSR storage formats. 

  One sparse matrix pattern which occasionally shows up in scientific and engineering applications has a full diagonal and a uniform random smattering of off-diagonals. This would be well-suited for PaddedSparse if the diagonal were to be extracted first and folded into the blocks of C using a vector dot product operation instead of a broadcast.


\subsection{Tensor-by-sparse multiplication}

Sparse-by-dense multiplication is significantly more difficult due to the asymmetry inherent in the outer-product formulation. One possible alternative is multiplication of sparse matrices with tensors. Breuer and Heinecke demonstrated with EDGE~\cite{10.1007/978-3-319-58667-0_3} that seismic simulations based on ADER-DG can be \emph{fused}, where ensembles of forward simulations are run in parallel within one execution. This has some immediate advantages. Read-only data structures can be shared, dramatically reducing memory movement. Meanwhile, the computations can be vectorized along the third dimension.

This formulation has some useful consequences for sparse kernels. Firstly, it eliminates inefficiencies caused by awkwardly sized data structures, such as matrices with 9 rows, and effortlessly ensures that memory accesses are aligned with cache line boundaries. Secondly, it gets away from the outer-product formulation: along the $m,n,k$ directions it resembles a naive matrix multiplication with no vectorization at all. This means that the sparse-by-tensor case can be unrolled just as well as the tensor-by-sparse case. 





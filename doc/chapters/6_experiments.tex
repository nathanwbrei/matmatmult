
\chapter{Experiments}
\label{chapter:experiments}


\section{Performance comparison of practical examples}

The goal of the first experiment is to show that the general approach can yield a speedup on matrices which are of practical interest. In the case of SeisSol, the compute kernels are recursively nested matrix multiplications of the pattern $Q' \mathrel{{+}{=}} K \cdot Q \cdot A$, where $K$ and $A$ are sparse and $Q$ is dense. The $A$ matrix always has the same shape and sparsity pattern, whereas there are many different matrices shaped like $K$. The performance of various kernels on the $Q \cdot A$ multiplication is depicted in Figure~\ref{fig:perf_seissol}. 

Unfortunately, the case of the $K \cdot Q$ multiplication can not be simply plugged in to SeisSol due to the asymmetry between dense and sparse multiplication discussed in Section X.XX. However, if the dense matrices were stored in row-major format instead of column-major, the situation would be reversed. Because the $K$ matrix is larger, this might yield a greater speedup on SeisSol as a whole. For these reasons the transposed problem $Q \cdot K$ is also considered in Figure~\ref{fig:perf_seissol_K}.


The experimental setup is straightforward. The problem size is set as $m=40,~n=15,~k=9$, corresponding to a viscoelastic rheological model as described in~\cite{7568431}. The sparsity pattern used is shown on the right in Figure~\ref{fig:seissol_star}. The only variable measured is wall clock time, which was used to calculate speedup relative to \py{libxsmm}'s dense kernel. The two new kernels tested are UnrolledSparse and GeneralSparse. The only available degree of freedom for matrices this small is the $m$-block size; the two extremes, $bm=8$ and $bm=40$, were both considered. The TiledSparse and BlockSparse kernels, on the other hand, were not considered because they would either degenerate to UnrolledSparse or produce complete fill-in, depending on the chosen block sizes $bn, bk$. The performance of Breuer's sparse kernels was also considered. 



  \begin{figure}[!htb]
    \centering
%    \begin{subfigure}[b]{0.4\textwidth}
%      \centering
      \includegraphics[width=0.75\textwidth]{images/seissol_comparison.pdf}
      \caption{Performance comparison of different spgemm kernels for the 9x15 SeisSol `star' matrix}
      \label{fig:perf_seissol}
%    \end{subfigure}
  \end{figure}


The experiment shows that the UnrolledSparse kernel with $bm=8$ has the best performance, corresponding to a speedup of $1.83$ over dense. To put this in context, the upper bound on the speedup for this problem is $(n\cdot k)/nnz = 3.97$. The UnrolledSparse with the larger block size performs slightly worse, at $1.74$. The GeneralSparse with the larger block size performed similarly worse than its UnrolledSparse equivalent, which was expected due to the indirect jump penalty. Meanwhile, the GeneralSparse with the small block size experienced a slowdown. 

The automatically generated Breuer kernel also experienced a slowdown relative to dense, which was not surprising due to the analysis in Section X.XX. However, an examination of the code revealed a bug: the SIMD vector length was being constrained to 32 bytes instead of 64. Manually fixing this led to a speedup of almost $1.5$ over dense, significantly better but still less than the UnrolledSparse kernel. The performance of the modified Breuer is expected to be similar for small matrices but then diverge as they get larger and denser, due to Breuer's lack of an accumulator for blocks of $C$. 

This experiment can be repeated for different patterns of $K$, and also for the $A$-matrix sizes $n \in \{27, 36\}$. 



\section{Choice of block sizes}

The previous section demonstrated the utility of UnrolledSparse kernel, and also showed that the choice of block sizes $bm,bn,bk$ affects the overall performance. The KNL architecture places a number of constraints on the optimal block size, which is described in depth for the dense case in~\ref{Heinecke:2016:LAS:3014904.3015017}. Since it is not entirely clear the extent to which the libxsmm results The goal of this experiment is to explore exactly 

While it does not affect the FLOP count, it certainly affects memory movement. 


What is the goal of this experiment?
What is the experimental setup?
What do the results show?



  \begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/unrolled_sizing_500_8.pdf}
      \caption{bm = 8, nnz = 500}
      \label{fig:unrolled_time}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/unrolled_sizing_500_16.pdf}
      \caption{bm = 16, nnz = 500}
      \label{fig:unrolled_perf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/unrolled_sizing_2000_8.pdf}
      \caption{bm = 8, nnz = 2000}
      \label{fig:unrolled_time}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/unrolled_sizing_2000_16.pdf}
      \caption{bm = 16, nnz = 2000}
      \label{fig:unrolled_perf}
    \end{subfigure}
    \caption{Effect of different block size choices}
  \end{figure}


\section{Scaling of UnrolledSparse}

  Matrix sizes: $m=128, n=28, k=128$.
  Microkernel block sizes: $bm=8, bn=28, bk=4$.
  B is gradually filled with randomly placed nonzeros.

  \begin{figure}[!htb]
  	\centering
    \begin{subfigure}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/fig2.pdf}
      \caption{Time to solution for unrolledsparse}
      \label{fig:unrolled_time}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/fig3.pdf}
      \caption{Performance of unrolledsparse}
      \label{fig:unrolled_perf}
    \end{subfigure}
  \end{figure}

  \begin{itemize}
    \item For $nnz < 3100$, time-to-solution scales linearly.
    \item For $nnz > 3200$, the FMAs completely fill the instruction cache (regardless of matrix dimensions!)
    \item Fully unrolled sparse algorithm outperforms dense ``nonzero'' GFlops, but always underperforms dense ``hardware'' GFlops
    
  \end{itemize} 


\section{Performance of GeneralSparse}

  \begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/jump_penalty.pdf}
      \caption{Small-scale }
      \label{fig:unrolled_time}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/jump_scaling.pdf}
      \caption{Large-scale performance}
      \label{fig:unrolled_perf}
    \end{subfigure}
  \end{figure}


\section{Tensor multiplication}




